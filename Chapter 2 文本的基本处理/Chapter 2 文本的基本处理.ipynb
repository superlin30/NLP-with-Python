{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"0\" width=\"100%\"><p align=\"left\"><img src=\"https://fic.swufe.edu.cn/img/logo_2.png\"  align=\"left\" width=30%></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Chapter 2 文本的基本处理</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**所有的NLP方法，无论是传统的机器学习方法还是深度学习方法，都是从一个包含原始文本的文本数据集开始，原始文本在python是一系列字符（字节），在中文语境下每一个字符为一个汉字。**\n",
    "###  本章节基本组织如下:\n",
    "* 基本的文本字符串处理方法\n",
    "* 文本清洗与正则表达式\n",
    "* 中文切词分词方法\n",
    "* 词频统计\n",
    "* 绘制词云图\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.基本的文本字符串处理方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 字符串（str）：由按照一定顺序组合在一起的字符构成\n",
    "str_text = \"原始文本的每一个字符为一个汉字。\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**字符串是一个序列，字符之间存在先后关系，所有字符都对应有一个索引，正向索引从左向右，由0开始递增，直到序列长度减1；反向索引从右向左，由-1开始递减，直到序列个数的负数值:**\n",
    "<img src=\"序列.png\" width=50%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "字符串的第一个词为: 原\n",
      "字符串的最后一个词为: 。\n",
      "字符串的第六到第九个字为: 每一个字\n"
     ]
    }
   ],
   "source": [
    "print(f\"字符串的第一个词为: {str_text[0]}\")  #序列的第一位索引从0开始\n",
    "print(f\"字符串的最后一个词为: {str_text[-1]}\")  #标点符号算一个字符\n",
    "print(f\"字符串的第六到第九个字为: {str_text[5:9]}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注：最后一种为字符串的分片（切片）操作，为左闭右开区间，例如:**\n",
    "    \n",
    "<img src=\"切片操作.png\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**在分片时，除了可以指定左边界和右边界的索引外，还可以增加第三个值，即步长：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc\n"
     ]
    }
   ],
   "source": [
    "# [开始:结束:步长]\n",
    "print(\"aaabbbccc\"[0:9:3])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**字符串为可迭代变量：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原!始!文!本!的!每!一!个!字!符!为!一!个!汉!字!。!\n"
     ]
    }
   ],
   "source": [
    "str_text2 =\"\"\n",
    "for i in str_text:\n",
    "    str_text2 += i+\"!\"\n",
    "print(str_text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**字符串基本操作符：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "True\n",
      "False\n",
      "原始文本的每一个字符为一个汉字。原始文本的每一个字符为一个汉字。\n",
      "原始文本的每一个字符为一个汉字。!!!!!!\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# str_text = \"原始文本的每一个字符为一个汉字。\"\n",
    "print(len(str_text))         #字符串长度\n",
    "print('原始文本' in str_text)  #判断\n",
    "print('汉字' not in str_text)\n",
    "print(str_text*2)            #复制\n",
    "print(str_text+\"!!!!!!\")     #连接\n",
    "print(str_text.count(\"一个\"))  #计数\n",
    "print(str_text.find(\"文本\"))  #返回查找内容开头的索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.文本清洗与正则表达式\n",
    "\n",
    "**现实中的文本数据中不仅包括了中文字符、数字、英文字符等字符，还存在一些网页乱码不需要的内容，需要进行处理:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('金融新闻_example.txt', 'r',encoding='utf-8') as text_file:   # 读取中文文本时要注意encoding\n",
    "    example = text_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<P class=Para>通讯社3月6日讯，在周三的投资者开放日活动中，聚飞光电（300303.SZ）董秘殷敬煌介绍，公司的传统优势业务是面向智能手机为主的智能终端产品的小尺寸背光LED；\\\\u3000\\\\u3000\\\\u3000而在小尺寸背光 LED 的基础上，公司向中大尺寸背光 LED 市场展开了扩张，同时积极布局潜力无限的照明LED市场。\\\\n\\\\n\\\\n\\\\n凭借着产品的高性价比、产业链精益化管理效率以及客户积累上的优势，2013年实现超过9亿元的发货额，净利润首次突破亿元。<P class=end>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'通讯社3月6日讯，在周三的投资者开放日活动中，聚飞光电（300303.SZ）董秘殷敬煌介绍，公司的传统优势业务是面向智能手机为主的智能终端产品的小尺寸背光LED；\\\\u3000\\\\u3000\\\\u3000而在小尺寸背光 LED 的基础上，公司向中大尺寸背光 LED 市场展开了扩张，同时积极布局潜力无限的照明LED市场。\\\\n\\\\n\\\\n\\\\n凭借着产品的高性价比、产业链精益化管理效率以及客户积累上的优势，2013年实现超过9亿元的发货额，净利润首次突破亿元。<P class=end>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.replace('<P class=Para>', '') # 将html标记<P class=Para>替换为空\n",
    "# <P class=Para>,<P class=end>难道不可以一起删除吗？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**简单的文本清洗可以用``replace``方法处理，但对于复杂结构的文本可以使用``正则表达式``来匹配对应的乱码并进行处理:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "通讯社3月6日讯，在周三的投资者开放日活动中，聚飞光电（300303.SZ）董秘殷敬煌介绍，公司的传统优势业务是面向智能手机为主的智能终端产品的小尺寸背光LED；\\u3000\\u3000\\u3000而在小尺寸背光 LED 的基础上，公司向中大尺寸背光 LED 市场展开了扩张，同时积极布局潜力无限的照明LED市场。\\n\\n\\n\\n凭借着产品的高性价比、产业链精益化管理效率以及客户积累上的优势，2013年实现超过9亿元的发货额，净利润首次突破亿元。\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "print(re.sub(r'(<.*?>)','',example))   #re.sub（r 查找的内容，替换的内容，传入文本）\n",
    "# <.*?>代表将左边界为<和右边界为>的文本内容进行匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "通讯社3月6日讯，在周三的投资者开放日活动中，聚飞光电（300303.SZ）董秘殷敬煌介绍，公司的传统优势业务是面向智能手机为主的智能终端产品的小尺寸背光LED；而在小尺寸背光 LED 的基础上，公司向中大尺寸背光 LED 市场展开了扩张，同时积极布局潜力无限的照明LED市场。凭借着产品的高性价比、产业链精益化管理效率以及客户积累上的优势，2013年实现超过9亿元的发货额，净利润首次突破亿元。\n"
     ]
    }
   ],
   "source": [
    "print(re.sub(r'(<.*?>)|(\\\\n)|\\\\u3000', '', example))\n",
    "# 可以利用()和|同时替换多个内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**常见正则表达式符号：**\n",
    "* ?匹配零次或一次前面的分组。\n",
    "* *匹配零次或多次前面的分组。\n",
    "* +匹配一次或多次前面的分组。\n",
    "* {n}匹配n 次前面的分组。\n",
    "* {n,}匹配n 次或更多前面的分组。\n",
    "* {,m}匹配零次到m 次前面的分组。\n",
    "* {n,m}匹配至少n 次、至多m 次前面的分组。\n",
    "* {n,m}?或*?或+?对前面的分组进行非贪心匹配。\n",
    "* ^hello 意味着字符串必须以hello开始。\n",
    "* hello$意味着字符串必须以hello结束。\n",
    "* .匹配所有字符，换行符除外。\n",
    "* \\d、\\w 和\\s 分别匹配数字、单词和空格。\n",
    "* \\D、\\W 和\\S 分别匹配除数字、单词和空格外的所有字符。\n",
    "* [abc]匹配方括号内的任意字符（诸如a、b 或c）。\n",
    "* [^abc]匹配不在方括号内的任意字符。\n",
    "\n",
    "**文本正则表达的匹配十分灵活多变，不用刻意去记忆，可以根据今后的具体问题再去查找对应的匹配方法**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.中文切词分词方法\n",
    "\n",
    "**将一个连续文本分解为词的过程称为分词，先考虑一个英文的情况：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'love', 'natural', 'language', 'processing', '.']\n"
     ]
    }
   ],
   "source": [
    "eng_text = 'I love natural language processing.'\n",
    "eng_text = eng_text.lower()              #所有字母转化为小写\n",
    "eng_text = eng_text.replace('.',' .')   #在句号前插入空格\n",
    "words = eng_text.split(' ')             #split方法将字符串按照引号内的内容(此例子中为空格)进行切分并传入一个list\n",
    "print(words)\n",
    "# 可以用正则表达式re.split('(\\w+)?',eng_text)直接实现，不必在句号前先插入空格"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**中文不像英文一样词和词之间是靠空格隔开的，单个的字并不能形成语义，将词确定下来是中文自然语言理解的第一步**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['中', '文', '以', '字', '为', '单', '位', '，', '单', '个', '的', '字', '连', '成', '词', '才', '能', '表', '达', '语', '义', '。']\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "cn_text = '中文以字为单位，单个的字连成词才能表达语义。'\n",
    "for i in cn_text:\n",
    "    words.append(i)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**调用``jieba``来进行中文分词:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\PC\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.643 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['中文', '以字', '为', '单位', '，', '单个', '的', '字', '连成', '词', '才能', '表达', '语义', '。']\n"
     ]
    }
   ],
   "source": [
    "import jieba # 若还未安装jieba，可以使用pip install jieba或conda install jieba（基于Anaconda）进行安装\n",
    "words = jieba.lcut(cn_text)   #默认为精确模式\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我', '今天', '学习', '了', '可', '接纳', '资产', '净值', '和', '累积', '优先股', '股息', '两个', '概念']\n"
     ]
    }
   ],
   "source": [
    "words = jieba.lcut('我今天学习了可接纳资产净值和累积优先股股息两个概念')   \n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**在实际情况中，可能会出现一些复杂专有词汇不想被切分开，可以调用`add_word`方法添加专有名词：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我', '今天', '学习', '了', '可接纳资产净值', '和', '累积', '优先股', '股息', '两个', '概念']\n"
     ]
    }
   ],
   "source": [
    "jieba.add_word('可接纳资产净值')  \n",
    "words = jieba.lcut('我今天学习了可接纳资产净值和累积优先股股息两个概念')  \n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**也可以使用`load_userdict`方法,批量添加用户自定义词典(UTF-8编码)：**\n",
    "<img src=\"jieba词典.jpg\" width=50%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我', '今天', '学习', '了', '可接纳资产净值', '和', '累积优先股股息', '两个', '概念']\n"
     ]
    }
   ],
   "source": [
    "jieba.load_userdict('jieba自定义词典_example.txt')\n",
    "words = jieba.lcut('我今天学习了可接纳资产净值和累积优先股股息两个概念')  \n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**更多关于jieba库的使用可以参考https://github.com/fxsjy/jieba ,其他的中文分词工具有：**\n",
    "* `THULAC`: https://github.com/thunlp/THULAC\n",
    "* `pkuseg`: https://github.com/lancopku/pkuseg-python\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.词频统计\n",
    "\n",
    "**词频统计任务会结合上述学习到的文本读取，清洗，分词等内容，接下来以一个例子进行回顾并进一步学习：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>自然语言理解和自然语言生成是自然语言处理的两大内核，机器翻译是自然语言理解方面最早的研究工作。<outer>自然语言处理的主要任务是：研究表示语言能力和语言应用的模型，建立和实现计算框架并提出相应的方法不断地完善模型，根据这样的语言模型设计有效地实现自然语言通信的计算机系统，并研讨关于系统的评测技术，最终实现用自然语言与计算机进行通信。目前，具有一定自然语言处理能力的典型应用包括计算机信息检索系统、多语种翻译系统等。语言是逻辑思维和交流的工具，宇宙万物中，只有人类才具有这种高级功能。要实现人与计算机间采用自然语言通信，必须使计算机同时具备自然语言理解和自然语言生成两大功能。因此，自然语言处理作为人工智能的一个子领域，其主要目的就包括两个方面：自然语言理解，让计算机理解自然语言文本的意义；自然语言生成，让计算机能以自然语言文本来表达给定的意图、思想等。自然语言是人类智慧的结晶，自然语言处理是人工智能中最为困难的问题之一，而对自然语言处理的研究也是充满魅力和挑战的。<P class=Para>近年来，自然语言处理处于快速发展阶段。各种词表、语义语法词典、语料库等数据资源的日益丰富，词语切分、词性标注、句法分析等技术的快速进步，各种新理论、新方法、新模型的出现推动了自然语言处理研究的繁荣。互联网与移动互联网和世界经济社会一体化的潮流对自然语言处理技术的迫切需求，为自然语言处理研究发展提供了强大的市场动力。我国直到上世纪80年代中期才开始较大规模和较系统的自然语言处理研究，尽管较国际水平尚有较大差距，但已经有了比较稳定的研究内容，包括语料库、知识库等数据资源建设，词语切分、句法分析等基础技术，以及信息检索、机器翻译等应用技术。当前国内外出现了一批基于自然语言处理技术的应用系统。自然语言处理的对象有词、句子、篇章和段落、文本等，但是大多归根到底在句子的处理上，自然语言处理中的自然语言句子级分析技术，可以大致分为词法分析、句法分析、语义分析三个层面。</html>\n"
     ]
    }
   ],
   "source": [
    "with open('词频统计example.txt', 'r',encoding='utf-8') as text_file:   # 读取txt文件\n",
    "    example = text_file.read()\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "自然语言理解和自然语言生成是自然语言处理的两大内核，机器翻译是自然语言理解方面最早的研究工作。自然语言处理的主要任务是：研究表示语言能力和语言应用的模型，建立和实现计算框架并提出相应的方法不断地完善模型，根据这样的语言模型设计有效地实现自然语言通信的计算机系统，并研讨关于系统的评测技术，最终实现用自然语言与计算机进行通信。目前，具有一定自然语言处理能力的典型应用包括计算机信息检索系统、多语种翻译系统等。语言是逻辑思维和交流的工具，宇宙万物中，只有人类才具有这种高级功能。要实现人与计算机间采用自然语言通信，必须使计算机同时具备自然语言理解和自然语言生成两大功能。因此，自然语言处理作为人工智能的一个子领域，其主要目的就包括两个方面：自然语言理解，让计算机理解自然语言文本的意义；自然语言生成，让计算机能以自然语言文本来表达给定的意图、思想等。自然语言是人类智慧的结晶，自然语言处理是人工智能中最为困难的问题之一，而对自然语言处理的研究也是充满魅力和挑战的。近年来，自然语言处理处于快速发展阶段。各种词表、语义语法词典、语料库等数据资源的日益丰富，词语切分、词性标注、句法分析等技术的快速进步，各种新理论、新方法、新模型的出现推动了自然语言处理研究的繁荣。互联网与移动互联网和世界经济社会一体化的潮流对自然语言处理技术的迫切需求，为自然语言处理研究发展提供了强大的市场动力。我国直到上世纪80年代中期才开始较大规模和较系统的自然语言处理研究，尽管较国际水平尚有较大差距，但已经有了比较稳定的研究内容，包括语料库、知识库等数据资源建设，词语切分、句法分析等基础技术，以及信息检索、机器翻译等应用技术。当前国内外出现了一批基于自然语言处理技术的应用系统。自然语言处理的对象有词、句子、篇章和段落、文本等，但是大多归根到底在句子的处理上，自然语言处理中的自然语言句子级分析技术，可以大致分为词法分析、句法分析、语义分析三个层面。\n"
     ]
    }
   ],
   "source": [
    "example = re.sub(r'(<.*?>)','',example)  #正则表达式清洗<>内容\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['自然语言', '理解', '和', '自然语言', '生成', '是', '自然语言处理', '的', '两大', '内核', '，', '机器翻译', '是', '自然语言', '理解', '方面', '最早', '的', '研究', '工作', '。', '自然语言处理', '的', '主要', '任务', '是', '：', '研究', '表示', '语言', '能力', '和', '语言', '应用', '的', '模型', '，', '建立', '和', '实现', '计算', '框架', '并', '提出', '相应', '的', '方法', '不断', '地', '完善']\n"
     ]
    }
   ],
   "source": [
    "jieba.add_word('自然语言处理')  \n",
    "words = jieba.lcut(example)   #jieba分词\n",
    "print(words[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**在进行词频分析时，一般我们会希望去除“的”、“是”以及标点符号等不重要的词汇，可以创建一个停用词典来删除对于研究并无实际贡献的词汇：**\n",
    "<img src=\"停用词典.jpg\" width=45%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['自然语言', '理解', '自然语言', '生成', '自然语言处理', '两大', '内核', '机器翻译', '自然语言', '理解', '方面', '最早', '研究', '工作', '自然语言处理', '主要', '任务', '研究', '表示', '语言', '能力', '语言', '应用', '模型', '建立', '实现', '计算', '框架', '提出', '相应', '方法', '不断', '完善', '模型', '根据', '这样', '语言', '模型', '设计', '有效', '实现', '自然语言', '通信', '计算机系统', '研讨', '关于', '系统', '评测', '技术', '最终']\n"
     ]
    }
   ],
   "source": [
    "with open('停用词典.txt', 'r',encoding='utf-8') as f:   # 停用词典\n",
    "    stopwords = [s.rstrip() for s in f.readlines()]     #.readlines()\n",
    "new_words = []\n",
    "for w in words:\n",
    "    if w not in stopwords:\n",
    "        new_words.append(w)\n",
    "print(new_words[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**词频统计：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总词数: 283\n",
      "不重复的词数: 183\n"
     ]
    }
   ],
   "source": [
    "print(f\"总词数: {len(new_words)}\") \n",
    "print(f\"不重复的词数: {len(set(new_words))}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "自然语言 14\n",
      "自然语言处理 14\n",
      "研究 7\n",
      "技术 7\n",
      "理解 5\n"
     ]
    }
   ],
   "source": [
    "counts = {}            #使用字典得到每个词出现的次数\n",
    "for word in new_words:\n",
    "    counts[word] =  counts.get(word,0)+1\n",
    "\n",
    "# 也可以直接调用函数Counter得到counts：\n",
    "# from collections import Counter\n",
    "# counts = Counter(new_words)\n",
    "\n",
    "counts = list(counts.items())   #转化为列表格式\n",
    "counts.sort(key = lambda x:x[1],reverse=True) #对词频进行排序\n",
    "for i in range(5):                  #展示出现次数最多的5个词\n",
    "    print(counts[i][0],counts[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.绘制词云图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud #若未安装，可以使用pip install wordcloud安装\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "自然语言 理解 自然语言 生成 自然语言处理 两大 内核 机器翻译 自然语言 理解 方面 最早 研究 工作 自然语言处理 主要 任务 研究 表示 语言 能力 语言 应用 模型 建立 实现 计算 框架 提出 相应 方法 不断 完善 模型 根据 这样 语言 模型 设计 有效 实现 自然语言 通信 计算机系统 研讨 关于 系统 评测 技术 最终 实现 用 自然语言 计算机 进行 通信 一定 自然语言处理 能力 典型 应用 包括 计算机信息 检索系统 多语种 翻译 系统 语言 逻辑思维 交流 工具 宇宙 万物 只有 人类 才 这种 高级 功能 要 实现 人 计算机 间 采用 自然语言 通信 必须 计算机 同时 具备 自然语言 理解 自然语言 生成 两大 功能 因此 自然语言处理 作为 人工智能 一个 子 领域 主要 目的 就 包括 两个 方面 自然语言 理解 让 计算机 理解 自然语言 文本 意义 ； 自然语言 生成 让 计算机 能以 自然语言 文 本来 表达 给定 意图 思想 自然语言 人类 智慧 结晶 自然语言处理 人工智能 最为 困难 问题 之一 而 对 自然语言处理 研究 也 充满 魅力 挑战 近年来 自然语言处理 处于 快速 发展 阶段 各种 词表 语义 语法 词典 语料库 数据 资源 日益 丰富 词语切分 词性 标注 句法分析 技术 快速 进步 各种 新 理论 新 方法 新 模型 出现 推动 自然语言处理 研究 繁荣 互联网 移动 互联网 世界 经济社会 一体化 潮流 对 自然语言处理 技术 迫切 需求 为 自然语言处理 研究 发展 提供 强大 市场 动力 我国 直到 世纪 80 年代 中期 才 开始 较 大规模 较 系统 自然语言处理 研究 尽管 较 国际 水平 尚有 较大 差距 已经 有 比较稳定 研究 内容 包括 语料库 知识库 数据 资源 建设 词语切分 句法分析 基础 技术 以及 信息检索 机器翻译 应用 技术 当前 国内外 出现 一批 基于 自然语言处理 技术 应用 系统 自然语言处理 对象 有词 句子 篇章 段落 文本 但是 大多 归根到底 在 句子 处理 自然语言处理 自然语言 句子 级 分析 技术 可以 大致 分为 词法 分析 句法分析 语义 分析 三个 层面\n"
     ]
    }
   ],
   "source": [
    "cloud_words =' '.join(new_words)   #词频统计里面的分词\n",
    "print(cloud_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x2524f55f5b0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc = WordCloud(\n",
    "    font_path='C:\\Windows\\Fonts\\msyh.ttc',  #在自己电脑的字体库中选择一款喜欢的字体，msyh.ttc为微软雅黑\n",
    "    background_color=\"white\",               #设置背景颜色\n",
    "    mask=imageio.imread('panda.png'),     #可以只用自己喜欢的png图片背景作为词云的形状\n",
    "    min_font_size=15,\n",
    "    margin=1,\n",
    "    max_words= 150\n",
    "                           \n",
    ")   #创建词云对象\n",
    "wc.generate(cloud_words)        #生成词云\n",
    "wc.to_file('wc.jpg')            #保存词云图片"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"wc.jpg\" width=70%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
