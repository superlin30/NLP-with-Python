{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"0\" width=\"100%\"><p align=\"left\"><img src=\"logo.png\"  align=\"left\" width=30%></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Chapter 4 词嵌入与word2vec</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**在前面几章中，我们已经学习了创建单词向量表示的传统方法，比如one-hot，词典。向量的长度都与词的个数有关，一般来说，文本越长，就需要用越长的向量来表示。有没有可能用一个低维的密集向量来表示长文本呢，这就是本章节的主要内容。**\n",
    "###  本章节基本组织如下:\n",
    "* 什么是词嵌入\n",
    "* word2vec方法（纯python与调包）\n",
    "* 利用PCA方法对词嵌入向量降至二维并可视化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.什么是词嵌入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**词嵌入：指把一个维数为所有词的数量的高维空间嵌入到一个维数低得多的连续向量空间中**\n",
    "<img src=\"词嵌入图1.jpg\" width=60%>\n",
    "**可以用一个简单的方法实现——矩阵乘法:假设one-hot向量长度为N，只需要与一个N*n的权重矩阵相乘，就可以降维为一个n维的低维向量**\n",
    "<img src=\"词嵌入图片2.png\" width=60%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01988013 0.02621099]\n",
      " [0.02830649 0.24621107]\n",
      " [0.86002795 0.53883106]\n",
      " [0.55282198 0.84203089]\n",
      " [0.12417332 0.27918368]\n",
      " [0.58575927 0.96959575]\n",
      " [0.56103022 0.01864729]\n",
      " [0.80063267 0.23297427]\n",
      " [0.8071052  0.38786064]\n",
      " [0.86354185 0.74712164]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "weight_matrix = np.random.rand(10, 2)  #生成符合服从“0-1”分布的随机权重矩阵(10*2)\n",
    "print(weight_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.52012428 2.08945919]\n"
     ]
    }
   ],
   "source": [
    "one_hot = [1,0,0,1,0,1,1,1,0,0]  # 1*10维one_hot文本向量（高维）\n",
    "print(np.dot(one_hot,weight_matrix))   #低维表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**在这里，权重矩阵是随机初始化的，得到的嵌入向量自然是毫无意义的，假如我们以某种方式通过训练得到了一个权重矩阵，可以捕获到嵌入向量之间的语法与语义关系，例如：**\n",
    "<img src=\"词嵌入图2.jpg\" width=40%>\n",
    "**而进一步扩展，神经网络模型也就是一个个节点相连的矩阵运算，word2vec，bert，GPT等等模型都是通过构造某种显式或隐式的辅助训练方法与特殊结构来捕获文本语料库语言属性的一个权重矩阵，而词嵌入，就可以理解为基于某种方法得到一个模型，利用该模型将稀疏的文本向量表示为富含语义的密集向量**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Word2Vec是一种简单并且计算高效的学习词嵌入的算法。Word2Vec的核心思想是学习一个神经网络语言模型来训练词向量，能够这些单词之间的关系。它基于分布式假设：上下文相似的词，其词嵌入向量也是相似的。word2vec使用推理的方法完成词嵌入\n",
    " <img src=\"https://pic2.zhimg.com/80/v2-28b59815b82157fecdd08bf88bce53a1_720w.jpg\" width=70%>\n",
    "word2vec分为两种模型：**\n",
    "* skip-gram：\n",
    "用当前词来预测上下文。相当于给你一个词，让你猜前面和后面可能出现什么词。\n",
    "* CBOW，continuous bag of words：\n",
    "通过上下文来预测当前值。相当于一句话中扣掉一个词，让你猜这个词是什么。\n",
    "<img src=\"https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fpic4.zhimg.com%2Fv2-e7bcb17222fbcf730e59c7fcfa2071e5_1200x500.jpg&refer=http%3A%2F%2Fpic4.zhimg.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=auto?sec=1664211820&t=eedd810083f38ed9bb83f55172042878\" width=70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1).纯python方法实现word2vec CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**把神经网络模型想象成搭积木，首先我们要定义全连接层（矩阵相乘）：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatMul:    #创建矩阵乘积的类，本质上就是np.dot，只不过加入了神经网络的反向传播\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):  #正向传播\n",
    "        W, = self.params\n",
    "        out = np.dot(x, W)\n",
    "        self.x = x\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):   #反向传播\n",
    "        W, = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        self.grads[0][...] = dW   #设置梯度权重\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.67435006 2.76383962]\n"
     ]
    }
   ],
   "source": [
    "one_hot = [1,0,0,1,0,1,1,1,0,0]  # 1*10维one_hot文本向量（高维）\n",
    "weight_matrix = np.random.rand(10, 2)  #生成符合服从“0-1”分布的随机权重矩阵(10*2)\n",
    "layer = MatMul(weight_matrix)   #创建全连接层\n",
    "print(layer.forward(one_hot))   #前向传播，也就是np.dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**通过全连接层完成CBOW的推理：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.24402117 -1.28061364  0.20446905  0.30276983 -0.46853567]]\n"
     ]
    }
   ],
   "source": [
    "# 假设我们的文本为“我 爱 你 中 国”\n",
    "c0 = np.array([[1, 0, 0, 0, 0]])   #c0代表“我”的onehot表示\n",
    "c2 = np.array([[0, 0, 1, 0, 0]])   #c2代表“你”的onehot表示\n",
    "\n",
    "# 初始化权重\n",
    "W_in = np.random.randn(5, 3)    #矩阵乘法规则：输入维度=输出维度\n",
    "W_out = np.random.randn(3, 5)\n",
    "\n",
    "# 生成层\n",
    "in_layer0 = MatMul(W_in)     \n",
    "in_layer1 = MatMul(W_in)\n",
    "out_layer = MatMul(W_out)\n",
    "\n",
    "# 正向传播\n",
    "h0 = in_layer0.forward(c0)   #c0通过全连接后得到的向量\n",
    "h2 = in_layer1.forward(c2)   #c1通过全连接后得到的向量\n",
    "h = 0.5 * (h0 + h2)          #相加取平均\n",
    "s = out_layer.forward(h)\n",
    "print(s)             #得到1*5向量，即我们想要去预测“我”和“你”之间应该是“爱”这个字的预测得分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**本质上预测中间的单词是一个在语料库上的多分类问题，因此得到预测得分后应该利用softmax函数将其转化为概率，在利用交叉熵计算loss：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):     #softmax\n",
    "    if x.ndim == 2:\n",
    "        x = x - x.max(axis=1, keepdims=True)\n",
    "        x = np.exp(x)\n",
    "        x /= x.sum(axis=1, keepdims=True)\n",
    "    elif x.ndim == 1:\n",
    "        x = x - np.max(x)\n",
    "        x = np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "    return x\n",
    "\n",
    "def cross_entropy_error(y, t):    #交叉熵\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 在监督标签为one-hot-vector的情况下，转换为正确解标签的索引\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测值: [[0.18358018 0.06510876 0.28747667 0.31717146 0.14666294]]\n",
      "真实值: [[0 1 0 0 0]]\n",
      "loss: 2.73169471409389\n"
     ]
    }
   ],
   "source": [
    "x = softmax(s)\n",
    "y = np.array([[0, 1,0, 0, 0]])   #c1代表“爱”的onehot表示，也就是我们最终想得到的真实值y，即label\n",
    "loss = cross_entropy_error(x, y)\n",
    "print(f\"预测值: {x}\")\n",
    "print(f\"真实值: {y}\")\n",
    "print(f\"loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**我们已经实现了CBOW的基本推理结构，现在只需要加入反向传播和梯度下降就可以实现模型的学习，在这之前先把刚才用到的代码打包成类，方便调用，并加入梯度下降优化器，文本处理，模型训练等亿点点细节（可跳过）：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class SimpleCBOW:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        V, H = vocab_size, hidden_size\n",
    "\n",
    "        # 初始化权重\n",
    "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        W_out = 0.01 * np.random.randn(H, V).astype('f')\n",
    "\n",
    "        # 生成层\n",
    "        self.in_layer0 = MatMul(W_in)\n",
    "        self.in_layer1 = MatMul(W_in)\n",
    "        self.out_layer = MatMul(W_out)\n",
    "        self.loss_layer = SoftmaxWithLoss()\n",
    "\n",
    "        # 将所有的权重和梯度整理到列表中\n",
    "        layers = [self.in_layer0, self.in_layer1, self.out_layer]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "        # 将单词的分布式表示设置为成员变量\n",
    "        self.word_vecs = W_in\n",
    "\n",
    "    def forward(self, contexts, target):\n",
    "        h0 = self.in_layer0.forward(contexts[:, 0])\n",
    "        h1 = self.in_layer1.forward(contexts[:, 1])\n",
    "        h = (h0 + h1) * 0.5\n",
    "        score = self.out_layer.forward(h)\n",
    "        loss = self.loss_layer.forward(score, target)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        ds = self.loss_layer.backward(dout)\n",
    "        da = self.out_layer.backward(ds)\n",
    "        da *= 0.5\n",
    "        self.in_layer1.backward(da)\n",
    "        self.in_layer0.backward(da)\n",
    "        return None\n",
    "    \n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.y = None  # softmax的输出\n",
    "        self.t = None  # 监督标签\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "\n",
    "        # 在监督标签为one-hot向量的情况下，转换为正确解标签的索引\n",
    "        if self.t.size == self.y.size:\n",
    "            self.t = self.t.argmax(axis=1)\n",
    "\n",
    "        loss = cross_entropy_error(self.y, self.t)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "\n",
    "        dx = self.y.copy()\n",
    "        dx[np.arange(batch_size), self.t] -= 1\n",
    "        dx *= dout\n",
    "        dx = dx / batch_size\n",
    "\n",
    "        return dx\n",
    "    \n",
    "def clip_grads(grads, max_norm):\n",
    "    total_norm = 0\n",
    "    for grad in grads:\n",
    "        total_norm += np.sum(grad ** 2)\n",
    "    total_norm = np.sqrt(total_norm)\n",
    "\n",
    "    rate = max_norm / (total_norm + 1e-6)\n",
    "    if rate < 1:\n",
    "        for grad in grads:\n",
    "            grad *= rate\n",
    "\n",
    "    \n",
    "class Trainer:   #模型训练类，可以跳过\n",
    "    def __init__(self, model, optimizer):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_list = []\n",
    "        self.eval_interval = None\n",
    "        self.current_epoch = 0\n",
    "\n",
    "    def fit(self, x, t, max_epoch=10, batch_size=32, max_grad=None, eval_interval=20):\n",
    "        data_size = len(x)\n",
    "        max_iters = data_size // batch_size\n",
    "        self.eval_interval = eval_interval\n",
    "        model, optimizer = self.model, self.optimizer\n",
    "        total_loss = 0\n",
    "        loss_count = 0\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in range(max_epoch):\n",
    "            # 打乱\n",
    "            idx = np.random.permutation(np.arange(data_size))\n",
    "            x = x[idx]\n",
    "            t = t[idx]\n",
    "\n",
    "            for iters in range(max_iters):\n",
    "                batch_x = x[iters*batch_size:(iters+1)*batch_size]\n",
    "                batch_t = t[iters*batch_size:(iters+1)*batch_size]\n",
    "\n",
    "                # 计算梯度，更新参数\n",
    "                loss = model.forward(batch_x, batch_t)\n",
    "                model.backward()\n",
    "                params, grads = remove_duplicate(model.params, model.grads)  # 将共享的权重整合为1个\n",
    "                if max_grad is not None:\n",
    "                    clip_grads(grads, max_grad)\n",
    "                optimizer.update(params, grads)\n",
    "                total_loss += loss\n",
    "                loss_count += 1\n",
    "\n",
    "                # 评价\n",
    "                if (eval_interval is not None) and (iters % eval_interval) == 0:\n",
    "                    avg_loss = total_loss / loss_count\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    print('| epoch %d |  iter %d / %d | time %d[s] | loss %.2f'\n",
    "                          % (self.current_epoch + 1, iters + 1, max_iters, elapsed_time, avg_loss))\n",
    "                    self.loss_list.append(float(avg_loss))\n",
    "                    total_loss, loss_count = 0, 0\n",
    "\n",
    "            self.current_epoch += 1\n",
    "\n",
    "    def plot(self, ylim=None):\n",
    "        x = np.arange(len(self.loss_list))\n",
    "        if ylim is not None:\n",
    "            plt.ylim(*ylim)\n",
    "        plt.plot(x, self.loss_list, label='train')\n",
    "        plt.xlabel('iterations (x' + str(self.eval_interval) + ')')\n",
    "        plt.ylabel('loss')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "class RnnlmTrainer:\n",
    "    def __init__(self, model, optimizer):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.time_idx = None\n",
    "        self.ppl_list = None\n",
    "        self.eval_interval = None\n",
    "        self.current_epoch = 0\n",
    "\n",
    "    def get_batch(self, x, t, batch_size, time_size):\n",
    "        batch_x = np.empty((batch_size, time_size), dtype='i')\n",
    "        batch_t = np.empty((batch_size, time_size), dtype='i')\n",
    "\n",
    "        data_size = len(x)\n",
    "        jump = data_size // batch_size\n",
    "        offsets = [i * jump for i in range(batch_size)]  # mini-batch的各笔样本数据的开始位置\n",
    "\n",
    "        for time in range(time_size):\n",
    "            for i, offset in enumerate(offsets):\n",
    "                batch_x[i, time] = x[(offset + self.time_idx) % data_size]\n",
    "                batch_t[i, time] = t[(offset + self.time_idx) % data_size]\n",
    "            self.time_idx += 1\n",
    "        return batch_x, batch_t\n",
    "\n",
    "    def fit(self, xs, ts, max_epoch=10, batch_size=20, time_size=35,\n",
    "            max_grad=None, eval_interval=20):\n",
    "        data_size = len(xs)\n",
    "        max_iters = data_size // (batch_size * time_size)\n",
    "        self.time_idx = 0\n",
    "        self.ppl_list = []\n",
    "        self.eval_interval = eval_interval\n",
    "        model, optimizer = self.model, self.optimizer\n",
    "        total_loss = 0\n",
    "        loss_count = 0\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in range(max_epoch):\n",
    "            for iters in range(max_iters):\n",
    "                batch_x, batch_t = self.get_batch(xs, ts, batch_size, time_size)\n",
    "\n",
    "                # 计算梯度，更新参数\n",
    "                loss = model.forward(batch_x, batch_t)\n",
    "                model.backward()\n",
    "                params, grads = remove_duplicate(model.params, model.grads)  # 将共享的权重整合为1个\n",
    "                if max_grad is not None:\n",
    "                    clip_grads(grads, max_grad)\n",
    "                optimizer.update(params, grads)\n",
    "                total_loss += loss\n",
    "                loss_count += 1\n",
    "\n",
    "                # 评价困惑度\n",
    "                if (eval_interval is not None) and (iters % eval_interval) == 0:\n",
    "                    ppl = np.exp(total_loss / loss_count)\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    print('| epoch %d |  iter %d / %d | time %d[s] | perplexity %.2f'\n",
    "                          % (self.current_epoch + 1, iters + 1, max_iters, elapsed_time, ppl))\n",
    "                    self.ppl_list.append(float(ppl))\n",
    "                    total_loss, loss_count = 0, 0\n",
    "\n",
    "            self.current_epoch += 1\n",
    "\n",
    "    def plot(self, ylim=None):\n",
    "        x = np.arange(len(self.ppl_list))\n",
    "        if ylim is not None:\n",
    "            plt.ylim(*ylim)\n",
    "        plt.plot(x, self.ppl_list, label='train')\n",
    "        plt.xlabel('iterations (x' + str(self.eval_interval) + ')')\n",
    "        plt.ylabel('perplexity')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def remove_duplicate(params, grads):\n",
    "    '''\n",
    "    将参数列表中重复的权重整合为1个，\n",
    "    加上与该权重对应的梯度\n",
    "    '''\n",
    "    params, grads = params[:], grads[:]  # copy list\n",
    "\n",
    "    while True:\n",
    "        find_flg = False\n",
    "        L = len(params)\n",
    "\n",
    "        for i in range(0, L - 1):\n",
    "            for j in range(i + 1, L):\n",
    "                # 在共享权重的情况下\n",
    "                if params[i] is params[j]:\n",
    "                    grads[i] += grads[j]  # 加上梯度\n",
    "                    find_flg = True\n",
    "                    params.pop(j)\n",
    "                    grads.pop(j)\n",
    "                # 在作为转置矩阵共享权重的情况下（weight tying）\n",
    "                elif params[i].ndim == 2 and params[j].ndim == 2 and \\\n",
    "                     params[i].T.shape == params[j].shape and np.all(params[i].T == params[j]):\n",
    "                    grads[i] += grads[j].T\n",
    "                    find_flg = True\n",
    "                    params.pop(j)\n",
    "                    grads.pop(j)\n",
    "\n",
    "                if find_flg: break\n",
    "            if find_flg: break\n",
    "\n",
    "        if not find_flg: break\n",
    "\n",
    "    return params, grads\n",
    "\n",
    "class Adam:\n",
    "    '''\n",
    "    Adam (http://arxiv.org/abs/1412.6980v8)\n",
    "    '''\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = [], []\n",
    "            for param in params:\n",
    "                self.m.append(np.zeros_like(param))\n",
    "                self.v.append(np.zeros_like(param))\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.m[i] += (1 - self.beta1) * (grads[i] - self.m[i])\n",
    "            self.v[i] += (1 - self.beta2) * (grads[i]**2 - self.v[i])\n",
    "            \n",
    "            params[i] -= lr_t * self.m[i] / (np.sqrt(self.v[i]) + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**训练我们自己的CBOW：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "向量表示: [0 1 2 3 4]\n",
      "字典: {0: '我', 1: '爱', 2: '你', 3: '中', 4: '国'}\n"
     ]
    }
   ],
   "source": [
    "#文本处理\n",
    "text = '我 爱 你 中 国'\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' .')\n",
    "    words = text.split(' ')\n",
    "\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "\n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "\n",
    "    return corpus, word_to_id, id_to_word\n",
    "\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "print(f\"向量表示: {corpus}\")\n",
    "print(f\"字典: {id_to_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [[[1 0 0 0 0]\n",
      "  [0 0 1 0 0]]\n",
      "\n",
      " [[0 1 0 0 0]\n",
      "  [0 0 0 1 0]]\n",
      "\n",
      " [[0 0 1 0 0]\n",
      "  [0 0 0 0 1]]]\n",
      "Y: [[0 1 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# 生成训练数据X,Y\n",
    "def convert_one_hot(corpus, vocab_size):\n",
    "    '''转换为one-hot表示\n",
    "    :param corpus: 单词ID列表（一维或二维的NumPy数组）\n",
    "    :param vocab_size: 词汇个数\n",
    "    :return: one-hot表示（二维或三维的NumPy数组）\n",
    "    '''\n",
    "    N = corpus.shape[0]\n",
    "\n",
    "    if corpus.ndim == 1:\n",
    "        one_hot = np.zeros((N, vocab_size), dtype=np.int32)\n",
    "        for idx, word_id in enumerate(corpus):\n",
    "            one_hot[idx, word_id] = 1\n",
    "\n",
    "    elif corpus.ndim == 2:\n",
    "        C = corpus.shape[1]\n",
    "        one_hot = np.zeros((N, C, vocab_size), dtype=np.int32)\n",
    "        for idx_0, word_ids in enumerate(corpus):\n",
    "            for idx_1, word_id in enumerate(word_ids):\n",
    "                one_hot[idx_0, idx_1, word_id] = 1\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "def create_contexts_target(corpus, window_size=1):\n",
    "    '''生成上下文和目标词\n",
    "    :param corpus: 语料库（单词ID列表）\n",
    "    :param window_size: 窗口大小（当窗口大小为1时，左右各1个单词为上下文）\n",
    "    :return:\n",
    "    '''\n",
    "    target = corpus[window_size:-window_size]\n",
    "    contexts = []\n",
    "\n",
    "    for idx in range(window_size, len(corpus)-window_size):\n",
    "        cs = []\n",
    "        for t in range(-window_size, window_size + 1):\n",
    "            if t == 0:\n",
    "                continue\n",
    "            cs.append(corpus[idx + t])\n",
    "        contexts.append(cs)\n",
    "\n",
    "    return np.array(contexts), np.array(target)\n",
    "\n",
    "window_size = 1\n",
    "vocab_size = len(word_to_id)\n",
    "contexts, target = create_contexts_target(corpus, window_size)\n",
    "target = convert_one_hot(target, vocab_size)\n",
    "contexts = convert_one_hot(contexts, vocab_size)\n",
    "print(f\"X: {contexts}\")\n",
    "print(f\"Y: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 |  iter 1 / 3 | time 0[s] | loss 1.61\n",
      "| epoch 2 |  iter 1 / 3 | time 0[s] | loss 1.61\n",
      "| epoch 3 |  iter 1 / 3 | time 0[s] | loss 1.61\n",
      "| epoch 4 |  iter 1 / 3 | time 0[s] | loss 1.61\n",
      "| epoch 5 |  iter 1 / 3 | time 0[s] | loss 1.61\n",
      "| epoch 6 |  iter 1 / 3 | time 0[s] | loss 1.61\n",
      "| epoch 7 |  iter 1 / 3 | time 0[s] | loss 1.61\n",
      "| epoch 8 |  iter 1 / 3 | time 0[s] | loss 1.61\n",
      "| epoch 9 |  iter 1 / 3 | time 0[s] | loss 1.61\n",
      "| epoch 10 |  iter 1 / 3 | time 0[s] | loss 1.61\n",
      "| epoch 11 |  iter 1 / 3 | time 0[s] | loss 1.61\n",
      "| epoch 12 |  iter 1 / 3 | time 0[s] | loss 1.61\n",
      "| epoch 13 |  iter 1 / 3 | time 0[s] | loss 1.61\n",
      "| epoch 14 |  iter 1 / 3 | time 0[s] | loss 1.61\n",
      "| epoch 15 |  iter 1 / 3 | time 0[s] | loss 1.61\n",
      "| epoch 16 |  iter 1 / 3 | time 0[s] | loss 1.61\n",
      "| epoch 17 |  iter 1 / 3 | time 0[s] | loss 1.61\n",
      "| epoch 18 |  iter 1 / 3 | time 0[s] | loss 1.61\n",
      "| epoch 19 |  iter 1 / 3 | time 0[s] | loss 1.60\n",
      "| epoch 20 |  iter 1 / 3 | time 0[s] | loss 1.60\n",
      "| epoch 21 |  iter 1 / 3 | time 0[s] | loss 1.60\n",
      "| epoch 22 |  iter 1 / 3 | time 0[s] | loss 1.60\n",
      "| epoch 23 |  iter 1 / 3 | time 0[s] | loss 1.60\n",
      "| epoch 24 |  iter 1 / 3 | time 0[s] | loss 1.60\n",
      "| epoch 25 |  iter 1 / 3 | time 0[s] | loss 1.60\n",
      "| epoch 26 |  iter 1 / 3 | time 0[s] | loss 1.60\n",
      "| epoch 27 |  iter 1 / 3 | time 0[s] | loss 1.60\n",
      "| epoch 28 |  iter 1 / 3 | time 0[s] | loss 1.60\n",
      "| epoch 29 |  iter 1 / 3 | time 0[s] | loss 1.60\n",
      "| epoch 30 |  iter 1 / 3 | time 0[s] | loss 1.60\n",
      "| epoch 31 |  iter 1 / 3 | time 0[s] | loss 1.60\n",
      "| epoch 32 |  iter 1 / 3 | time 0[s] | loss 1.59\n",
      "| epoch 33 |  iter 1 / 3 | time 0[s] | loss 1.59\n",
      "| epoch 34 |  iter 1 / 3 | time 0[s] | loss 1.60\n",
      "| epoch 35 |  iter 1 / 3 | time 0[s] | loss 1.59\n",
      "| epoch 36 |  iter 1 / 3 | time 0[s] | loss 1.59\n",
      "| epoch 37 |  iter 1 / 3 | time 0[s] | loss 1.59\n",
      "| epoch 38 |  iter 1 / 3 | time 0[s] | loss 1.59\n",
      "| epoch 39 |  iter 1 / 3 | time 0[s] | loss 1.59\n",
      "| epoch 40 |  iter 1 / 3 | time 0[s] | loss 1.59\n",
      "| epoch 41 |  iter 1 / 3 | time 0[s] | loss 1.58\n",
      "| epoch 42 |  iter 1 / 3 | time 0[s] | loss 1.58\n",
      "| epoch 43 |  iter 1 / 3 | time 0[s] | loss 1.58\n",
      "| epoch 44 |  iter 1 / 3 | time 0[s] | loss 1.57\n",
      "| epoch 45 |  iter 1 / 3 | time 0[s] | loss 1.58\n",
      "| epoch 46 |  iter 1 / 3 | time 0[s] | loss 1.57\n",
      "| epoch 47 |  iter 1 / 3 | time 0[s] | loss 1.57\n",
      "| epoch 48 |  iter 1 / 3 | time 0[s] | loss 1.57\n",
      "| epoch 49 |  iter 1 / 3 | time 0[s] | loss 1.58\n",
      "| epoch 50 |  iter 1 / 3 | time 0[s] | loss 1.56\n",
      "| epoch 51 |  iter 1 / 3 | time 0[s] | loss 1.56\n",
      "| epoch 52 |  iter 1 / 3 | time 0[s] | loss 1.57\n",
      "| epoch 53 |  iter 1 / 3 | time 0[s] | loss 1.56\n",
      "| epoch 54 |  iter 1 / 3 | time 0[s] | loss 1.55\n",
      "| epoch 55 |  iter 1 / 3 | time 0[s] | loss 1.57\n",
      "| epoch 56 |  iter 1 / 3 | time 0[s] | loss 1.54\n",
      "| epoch 57 |  iter 1 / 3 | time 0[s] | loss 1.55\n",
      "| epoch 58 |  iter 1 / 3 | time 0[s] | loss 1.56\n",
      "| epoch 59 |  iter 1 / 3 | time 0[s] | loss 1.55\n",
      "| epoch 60 |  iter 1 / 3 | time 0[s] | loss 1.54\n",
      "| epoch 61 |  iter 1 / 3 | time 0[s] | loss 1.56\n",
      "| epoch 62 |  iter 1 / 3 | time 0[s] | loss 1.54\n",
      "| epoch 63 |  iter 1 / 3 | time 0[s] | loss 1.53\n",
      "| epoch 64 |  iter 1 / 3 | time 0[s] | loss 1.53\n",
      "| epoch 65 |  iter 1 / 3 | time 0[s] | loss 1.53\n",
      "| epoch 66 |  iter 1 / 3 | time 0[s] | loss 1.53\n",
      "| epoch 67 |  iter 1 / 3 | time 0[s] | loss 1.53\n",
      "| epoch 68 |  iter 1 / 3 | time 0[s] | loss 1.52\n",
      "| epoch 69 |  iter 1 / 3 | time 0[s] | loss 1.53\n",
      "| epoch 70 |  iter 1 / 3 | time 0[s] | loss 1.50\n",
      "| epoch 71 |  iter 1 / 3 | time 0[s] | loss 1.52\n",
      "| epoch 72 |  iter 1 / 3 | time 0[s] | loss 1.53\n",
      "| epoch 73 |  iter 1 / 3 | time 0[s] | loss 1.52\n",
      "| epoch 74 |  iter 1 / 3 | time 0[s] | loss 1.49\n",
      "| epoch 75 |  iter 1 / 3 | time 0[s] | loss 1.52\n",
      "| epoch 76 |  iter 1 / 3 | time 0[s] | loss 1.50\n",
      "| epoch 77 |  iter 1 / 3 | time 0[s] | loss 1.48\n",
      "| epoch 78 |  iter 1 / 3 | time 0[s] | loss 1.51\n",
      "| epoch 79 |  iter 1 / 3 | time 0[s] | loss 1.48\n",
      "| epoch 80 |  iter 1 / 3 | time 0[s] | loss 1.49\n",
      "| epoch 81 |  iter 1 / 3 | time 0[s] | loss 1.49\n",
      "| epoch 82 |  iter 1 / 3 | time 0[s] | loss 1.48\n",
      "| epoch 83 |  iter 1 / 3 | time 0[s] | loss 1.50\n",
      "| epoch 84 |  iter 1 / 3 | time 0[s] | loss 1.48\n",
      "| epoch 85 |  iter 1 / 3 | time 0[s] | loss 1.47\n",
      "| epoch 86 |  iter 1 / 3 | time 0[s] | loss 1.45\n",
      "| epoch 87 |  iter 1 / 3 | time 0[s] | loss 1.47\n",
      "| epoch 88 |  iter 1 / 3 | time 0[s] | loss 1.46\n",
      "| epoch 89 |  iter 1 / 3 | time 0[s] | loss 1.49\n",
      "| epoch 90 |  iter 1 / 3 | time 0[s] | loss 1.43\n",
      "| epoch 91 |  iter 1 / 3 | time 0[s] | loss 1.48\n",
      "| epoch 92 |  iter 1 / 3 | time 0[s] | loss 1.42\n",
      "| epoch 93 |  iter 1 / 3 | time 0[s] | loss 1.44\n",
      "| epoch 94 |  iter 1 / 3 | time 0[s] | loss 1.47\n",
      "| epoch 95 |  iter 1 / 3 | time 0[s] | loss 1.44\n",
      "| epoch 96 |  iter 1 / 3 | time 0[s] | loss 1.42\n",
      "| epoch 97 |  iter 1 / 3 | time 0[s] | loss 1.43\n",
      "| epoch 98 |  iter 1 / 3 | time 0[s] | loss 1.44\n",
      "| epoch 99 |  iter 1 / 3 | time 0[s] | loss 1.39\n",
      "| epoch 100 |  iter 1 / 3 | time 0[s] | loss 1.45\n",
      "| epoch 101 |  iter 1 / 3 | time 0[s] | loss 1.40\n",
      "| epoch 102 |  iter 1 / 3 | time 0[s] | loss 1.42\n",
      "| epoch 103 |  iter 1 / 3 | time 0[s] | loss 1.37\n",
      "| epoch 104 |  iter 1 / 3 | time 0[s] | loss 1.43\n",
      "| epoch 105 |  iter 1 / 3 | time 0[s] | loss 1.40\n",
      "| epoch 106 |  iter 1 / 3 | time 0[s] | loss 1.39\n",
      "| epoch 107 |  iter 1 / 3 | time 0[s] | loss 1.36\n",
      "| epoch 108 |  iter 1 / 3 | time 0[s] | loss 1.39\n",
      "| epoch 109 |  iter 1 / 3 | time 0[s] | loss 1.41\n",
      "| epoch 110 |  iter 1 / 3 | time 0[s] | loss 1.39\n",
      "| epoch 111 |  iter 1 / 3 | time 0[s] | loss 1.37\n",
      "| epoch 112 |  iter 1 / 3 | time 0[s] | loss 1.33\n",
      "| epoch 113 |  iter 1 / 3 | time 0[s] | loss 1.37\n",
      "| epoch 114 |  iter 1 / 3 | time 0[s] | loss 1.41\n",
      "| epoch 115 |  iter 1 / 3 | time 0[s] | loss 1.36\n",
      "| epoch 116 |  iter 1 / 3 | time 0[s] | loss 1.31\n",
      "| epoch 117 |  iter 1 / 3 | time 0[s] | loss 1.38\n",
      "| epoch 118 |  iter 1 / 3 | time 0[s] | loss 1.36\n",
      "| epoch 119 |  iter 1 / 3 | time 0[s] | loss 1.29\n",
      "| epoch 120 |  iter 1 / 3 | time 0[s] | loss 1.34\n",
      "| epoch 121 |  iter 1 / 3 | time 0[s] | loss 1.38\n",
      "| epoch 122 |  iter 1 / 3 | time 0[s] | loss 1.31\n",
      "| epoch 123 |  iter 1 / 3 | time 0[s] | loss 1.34\n",
      "| epoch 124 |  iter 1 / 3 | time 0[s] | loss 1.27\n",
      "| epoch 125 |  iter 1 / 3 | time 0[s] | loss 1.37\n",
      "| epoch 126 |  iter 1 / 3 | time 0[s] | loss 1.31\n",
      "| epoch 127 |  iter 1 / 3 | time 0[s] | loss 1.31\n",
      "| epoch 128 |  iter 1 / 3 | time 0[s] | loss 1.28\n",
      "| epoch 129 |  iter 1 / 3 | time 0[s] | loss 1.31\n",
      "| epoch 130 |  iter 1 / 3 | time 0[s] | loss 1.24\n",
      "| epoch 131 |  iter 1 / 3 | time 0[s] | loss 1.33\n",
      "| epoch 132 |  iter 1 / 3 | time 0[s] | loss 1.24\n",
      "| epoch 133 |  iter 1 / 3 | time 0[s] | loss 1.34\n",
      "| epoch 134 |  iter 1 / 3 | time 0[s] | loss 1.25\n",
      "| epoch 135 |  iter 1 / 3 | time 0[s] | loss 1.23\n",
      "| epoch 136 |  iter 1 / 3 | time 0[s] | loss 1.32\n",
      "| epoch 137 |  iter 1 / 3 | time 0[s] | loss 1.20\n",
      "| epoch 138 |  iter 1 / 3 | time 0[s] | loss 1.30\n",
      "| epoch 139 |  iter 1 / 3 | time 0[s] | loss 1.25\n",
      "| epoch 140 |  iter 1 / 3 | time 0[s] | loss 1.20\n",
      "| epoch 141 |  iter 1 / 3 | time 0[s] | loss 1.24\n",
      "| epoch 142 |  iter 1 / 3 | time 0[s] | loss 1.28\n",
      "| epoch 143 |  iter 1 / 3 | time 0[s] | loss 1.19\n",
      "| epoch 144 |  iter 1 / 3 | time 0[s] | loss 1.29\n",
      "| epoch 145 |  iter 1 / 3 | time 0[s] | loss 1.22\n",
      "| epoch 146 |  iter 1 / 3 | time 0[s] | loss 1.15\n",
      "| epoch 147 |  iter 1 / 3 | time 0[s] | loss 1.26\n",
      "| epoch 148 |  iter 1 / 3 | time 0[s] | loss 1.16\n",
      "| epoch 149 |  iter 1 / 3 | time 0[s] | loss 1.25\n",
      "| epoch 150 |  iter 1 / 3 | time 0[s] | loss 1.15\n",
      "| epoch 151 |  iter 1 / 3 | time 0[s] | loss 1.24\n",
      "| epoch 152 |  iter 1 / 3 | time 0[s] | loss 1.14\n",
      "| epoch 153 |  iter 1 / 3 | time 0[s] | loss 1.23\n",
      "| epoch 154 |  iter 1 / 3 | time 0[s] | loss 1.13\n",
      "| epoch 155 |  iter 1 / 3 | time 0[s] | loss 1.22\n",
      "| epoch 156 |  iter 1 / 3 | time 0[s] | loss 1.19\n",
      "| epoch 157 |  iter 1 / 3 | time 0[s] | loss 1.14\n",
      "| epoch 158 |  iter 1 / 3 | time 0[s] | loss 1.18\n",
      "| epoch 159 |  iter 1 / 3 | time 0[s] | loss 1.08\n",
      "| epoch 160 |  iter 1 / 3 | time 0[s] | loss 1.22\n",
      "| epoch 161 |  iter 1 / 3 | time 0[s] | loss 1.07\n",
      "| epoch 162 |  iter 1 / 3 | time 0[s] | loss 1.19\n",
      "| epoch 163 |  iter 1 / 3 | time 0[s] | loss 1.14\n",
      "| epoch 164 |  iter 1 / 3 | time 0[s] | loss 1.16\n",
      "| epoch 165 |  iter 1 / 3 | time 0[s] | loss 1.13\n",
      "| epoch 166 |  iter 1 / 3 | time 0[s] | loss 1.12\n",
      "| epoch 167 |  iter 1 / 3 | time 0[s] | loss 1.12\n",
      "| epoch 168 |  iter 1 / 3 | time 0[s] | loss 1.03\n",
      "| epoch 169 |  iter 1 / 3 | time 0[s] | loss 1.19\n",
      "| epoch 170 |  iter 1 / 3 | time 0[s] | loss 1.08\n",
      "| epoch 171 |  iter 1 / 3 | time 0[s] | loss 1.12\n",
      "| epoch 172 |  iter 1 / 3 | time 0[s] | loss 1.09\n",
      "| epoch 173 |  iter 1 / 3 | time 0[s] | loss 1.06\n",
      "| epoch 174 |  iter 1 / 3 | time 0[s] | loss 1.11\n",
      "| epoch 175 |  iter 1 / 3 | time 0[s] | loss 1.08\n",
      "| epoch 176 |  iter 1 / 3 | time 0[s] | loss 1.07\n",
      "| epoch 177 |  iter 1 / 3 | time 0[s] | loss 1.04\n",
      "| epoch 178 |  iter 1 / 3 | time 0[s] | loss 1.09\n",
      "| epoch 179 |  iter 1 / 3 | time 0[s] | loss 0.97\n",
      "| epoch 180 |  iter 1 / 3 | time 0[s] | loss 1.05\n",
      "| epoch 181 |  iter 1 / 3 | time 0[s] | loss 1.13\n",
      "| epoch 182 |  iter 1 / 3 | time 0[s] | loss 1.02\n",
      "| epoch 183 |  iter 1 / 3 | time 0[s] | loss 0.98\n",
      "| epoch 184 |  iter 1 / 3 | time 0[s] | loss 1.03\n",
      "| epoch 185 |  iter 1 / 3 | time 0[s] | loss 1.09\n",
      "| epoch 186 |  iter 1 / 3 | time 0[s] | loss 1.05\n",
      "| epoch 187 |  iter 1 / 3 | time 0[s] | loss 0.93\n",
      "| epoch 188 |  iter 1 / 3 | time 0[s] | loss 1.07\n",
      "| epoch 189 |  iter 1 / 3 | time 0[s] | loss 1.01\n",
      "| epoch 190 |  iter 1 / 3 | time 0[s] | loss 1.03\n",
      "| epoch 191 |  iter 1 / 3 | time 0[s] | loss 1.00\n",
      "| epoch 192 |  iter 1 / 3 | time 0[s] | loss 0.97\n",
      "| epoch 193 |  iter 1 / 3 | time 0[s] | loss 0.93\n",
      "| epoch 194 |  iter 1 / 3 | time 0[s] | loss 0.99\n",
      "| epoch 195 |  iter 1 / 3 | time 0[s] | loss 1.04\n",
      "| epoch 196 |  iter 1 / 3 | time 0[s] | loss 0.91\n",
      "| epoch 197 |  iter 1 / 3 | time 0[s] | loss 0.97\n",
      "| epoch 198 |  iter 1 / 3 | time 0[s] | loss 1.06\n",
      "| epoch 199 |  iter 1 / 3 | time 0[s] | loss 0.87\n",
      "| epoch 200 |  iter 1 / 3 | time 0[s] | loss 1.02\n",
      "| epoch 201 |  iter 1 / 3 | time 0[s] | loss 0.98\n",
      "| epoch 202 |  iter 1 / 3 | time 0[s] | loss 0.92\n",
      "| epoch 203 |  iter 1 / 3 | time 0[s] | loss 0.94\n",
      "| epoch 204 |  iter 1 / 3 | time 0[s] | loss 0.94\n",
      "| epoch 205 |  iter 1 / 3 | time 0[s] | loss 0.96\n",
      "| epoch 206 |  iter 1 / 3 | time 0[s] | loss 0.90\n",
      "| epoch 207 |  iter 1 / 3 | time 0[s] | loss 0.86\n",
      "| epoch 208 |  iter 1 / 3 | time 0[s] | loss 0.92\n",
      "| epoch 209 |  iter 1 / 3 | time 0[s] | loss 1.01\n",
      "| epoch 210 |  iter 1 / 3 | time 0[s] | loss 0.91\n",
      "| epoch 211 |  iter 1 / 3 | time 0[s] | loss 0.88\n",
      "| epoch 212 |  iter 1 / 3 | time 0[s] | loss 0.90\n",
      "| epoch 213 |  iter 1 / 3 | time 0[s] | loss 0.90\n",
      "| epoch 214 |  iter 1 / 3 | time 0[s] | loss 0.83\n",
      "| epoch 215 |  iter 1 / 3 | time 0[s] | loss 0.89\n",
      "| epoch 216 |  iter 1 / 3 | time 0[s] | loss 0.95\n",
      "| epoch 217 |  iter 1 / 3 | time 0[s] | loss 0.88\n",
      "| epoch 218 |  iter 1 / 3 | time 0[s] | loss 0.81\n",
      "| epoch 219 |  iter 1 / 3 | time 0[s] | loss 0.96\n",
      "| epoch 220 |  iter 1 / 3 | time 0[s] | loss 0.83\n",
      "| epoch 221 |  iter 1 / 3 | time 0[s] | loss 0.89\n",
      "| epoch 222 |  iter 1 / 3 | time 0[s] | loss 0.76\n",
      "| epoch 223 |  iter 1 / 3 | time 0[s] | loss 0.85\n",
      "| epoch 224 |  iter 1 / 3 | time 0[s] | loss 0.85\n",
      "| epoch 225 |  iter 1 / 3 | time 0[s] | loss 0.94\n",
      "| epoch 226 |  iter 1 / 3 | time 0[s] | loss 0.81\n",
      "| epoch 227 |  iter 1 / 3 | time 0[s] | loss 0.86\n",
      "| epoch 228 |  iter 1 / 3 | time 0[s] | loss 0.80\n",
      "| epoch 229 |  iter 1 / 3 | time 0[s] | loss 0.85\n",
      "| epoch 230 |  iter 1 / 3 | time 0[s] | loss 0.82\n",
      "| epoch 231 |  iter 1 / 3 | time 0[s] | loss 0.72\n",
      "| epoch 232 |  iter 1 / 3 | time 0[s] | loss 0.88\n",
      "| epoch 233 |  iter 1 / 3 | time 0[s] | loss 0.74\n",
      "| epoch 234 |  iter 1 / 3 | time 0[s] | loss 0.87\n",
      "| epoch 235 |  iter 1 / 3 | time 0[s] | loss 0.80\n",
      "| epoch 236 |  iter 1 / 3 | time 0[s] | loss 0.73\n",
      "| epoch 237 |  iter 1 / 3 | time 0[s] | loss 0.88\n",
      "| epoch 238 |  iter 1 / 3 | time 0[s] | loss 0.69\n",
      "| epoch 239 |  iter 1 / 3 | time 0[s] | loss 0.78\n",
      "| epoch 240 |  iter 1 / 3 | time 0[s] | loss 0.84\n",
      "| epoch 241 |  iter 1 / 3 | time 0[s] | loss 0.77\n",
      "| epoch 242 |  iter 1 / 3 | time 0[s] | loss 0.70\n",
      "| epoch 243 |  iter 1 / 3 | time 0[s] | loss 0.76\n",
      "| epoch 244 |  iter 1 / 3 | time 0[s] | loss 0.76\n",
      "| epoch 245 |  iter 1 / 3 | time 0[s] | loss 0.82\n",
      "| epoch 246 |  iter 1 / 3 | time 0[s] | loss 0.75\n",
      "| epoch 247 |  iter 1 / 3 | time 0[s] | loss 0.75\n",
      "| epoch 248 |  iter 1 / 3 | time 0[s] | loss 0.74\n",
      "| epoch 249 |  iter 1 / 3 | time 0[s] | loss 0.74\n",
      "| epoch 250 |  iter 1 / 3 | time 0[s] | loss 0.73\n",
      "| epoch 251 |  iter 1 / 3 | time 0[s] | loss 0.66\n",
      "| epoch 252 |  iter 1 / 3 | time 0[s] | loss 0.79\n",
      "| epoch 253 |  iter 1 / 3 | time 0[s] | loss 0.72\n",
      "| epoch 254 |  iter 1 / 3 | time 0[s] | loss 0.75\n",
      "| epoch 255 |  iter 1 / 3 | time 0[s] | loss 0.68\n",
      "| epoch 256 |  iter 1 / 3 | time 0[s] | loss 0.64\n",
      "| epoch 257 |  iter 1 / 3 | time 0[s] | loss 0.80\n",
      "| epoch 258 |  iter 1 / 3 | time 0[s] | loss 0.70\n",
      "| epoch 259 |  iter 1 / 3 | time 0[s] | loss 0.67\n",
      "| epoch 260 |  iter 1 / 3 | time 0[s] | loss 0.72\n",
      "| epoch 261 |  iter 1 / 3 | time 0[s] | loss 0.60\n",
      "| epoch 262 |  iter 1 / 3 | time 0[s] | loss 0.68\n",
      "| epoch 263 |  iter 1 / 3 | time 0[s] | loss 0.68\n",
      "| epoch 264 |  iter 1 / 3 | time 0[s] | loss 0.68\n",
      "| epoch 265 |  iter 1 / 3 | time 0[s] | loss 0.74\n",
      "| epoch 266 |  iter 1 / 3 | time 0[s] | loss 0.60\n",
      "| epoch 267 |  iter 1 / 3 | time 0[s] | loss 0.76\n",
      "| epoch 268 |  iter 1 / 3 | time 0[s] | loss 0.63\n",
      "| epoch 269 |  iter 1 / 3 | time 0[s] | loss 0.68\n",
      "| epoch 270 |  iter 1 / 3 | time 0[s] | loss 0.56\n",
      "| epoch 271 |  iter 1 / 3 | time 0[s] | loss 0.65\n",
      "| epoch 272 |  iter 1 / 3 | time 0[s] | loss 0.74\n",
      "| epoch 273 |  iter 1 / 3 | time 0[s] | loss 0.62\n",
      "| epoch 274 |  iter 1 / 3 | time 0[s] | loss 0.57\n",
      "| epoch 275 |  iter 1 / 3 | time 0[s] | loss 0.73\n",
      "| epoch 276 |  iter 1 / 3 | time 0[s] | loss 0.61\n",
      "| epoch 277 |  iter 1 / 3 | time 0[s] | loss 0.65\n",
      "| epoch 278 |  iter 1 / 3 | time 0[s] | loss 0.62\n",
      "| epoch 279 |  iter 1 / 3 | time 0[s] | loss 0.53\n",
      "| epoch 280 |  iter 1 / 3 | time 0[s] | loss 0.68\n",
      "| epoch 281 |  iter 1 / 3 | time 0[s] | loss 0.64\n",
      "| epoch 282 |  iter 1 / 3 | time 0[s] | loss 0.52\n",
      "| epoch 283 |  iter 1 / 3 | time 0[s] | loss 0.67\n",
      "| epoch 284 |  iter 1 / 3 | time 0[s] | loss 0.63\n",
      "| epoch 285 |  iter 1 / 3 | time 0[s] | loss 0.57\n",
      "| epoch 286 |  iter 1 / 3 | time 0[s] | loss 0.62\n",
      "| epoch 287 |  iter 1 / 3 | time 0[s] | loss 0.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 288 |  iter 1 / 3 | time 0[s] | loss 0.65\n",
      "| epoch 289 |  iter 1 / 3 | time 0[s] | loss 0.61\n",
      "| epoch 290 |  iter 1 / 3 | time 0[s] | loss 0.58\n",
      "| epoch 291 |  iter 1 / 3 | time 0[s] | loss 0.49\n",
      "| epoch 292 |  iter 1 / 3 | time 0[s] | loss 0.57\n",
      "| epoch 293 |  iter 1 / 3 | time 0[s] | loss 0.63\n",
      "| epoch 294 |  iter 1 / 3 | time 0[s] | loss 0.57\n",
      "| epoch 295 |  iter 1 / 3 | time 0[s] | loss 0.59\n",
      "| epoch 296 |  iter 1 / 3 | time 0[s] | loss 0.56\n",
      "| epoch 297 |  iter 1 / 3 | time 0[s] | loss 0.56\n",
      "| epoch 298 |  iter 1 / 3 | time 0[s] | loss 0.53\n",
      "| epoch 299 |  iter 1 / 3 | time 0[s] | loss 0.55\n",
      "| epoch 300 |  iter 1 / 3 | time 0[s] | loss 0.57\n",
      "| epoch 301 |  iter 1 / 3 | time 0[s] | loss 0.52\n",
      "| epoch 302 |  iter 1 / 3 | time 0[s] | loss 0.56\n",
      "| epoch 303 |  iter 1 / 3 | time 0[s] | loss 0.54\n",
      "| epoch 304 |  iter 1 / 3 | time 0[s] | loss 0.45\n",
      "| epoch 305 |  iter 1 / 3 | time 0[s] | loss 0.53\n",
      "| epoch 306 |  iter 1 / 3 | time 0[s] | loss 0.59\n",
      "| epoch 307 |  iter 1 / 3 | time 0[s] | loss 0.53\n",
      "| epoch 308 |  iter 1 / 3 | time 0[s] | loss 0.54\n",
      "| epoch 309 |  iter 1 / 3 | time 0[s] | loss 0.52\n",
      "| epoch 310 |  iter 1 / 3 | time 0[s] | loss 0.44\n",
      "| epoch 311 |  iter 1 / 3 | time 0[s] | loss 0.51\n",
      "| epoch 312 |  iter 1 / 3 | time 0[s] | loss 0.51\n",
      "| epoch 313 |  iter 1 / 3 | time 0[s] | loss 0.59\n",
      "| epoch 314 |  iter 1 / 3 | time 0[s] | loss 0.43\n",
      "| epoch 315 |  iter 1 / 3 | time 0[s] | loss 0.56\n",
      "| epoch 316 |  iter 1 / 3 | time 0[s] | loss 0.52\n",
      "| epoch 317 |  iter 1 / 3 | time 0[s] | loss 0.49\n",
      "| epoch 318 |  iter 1 / 3 | time 0[s] | loss 0.47\n",
      "| epoch 319 |  iter 1 / 3 | time 0[s] | loss 0.49\n",
      "| epoch 320 |  iter 1 / 3 | time 0[s] | loss 0.49\n",
      "| epoch 321 |  iter 1 / 3 | time 0[s] | loss 0.43\n",
      "| epoch 322 |  iter 1 / 3 | time 0[s] | loss 0.56\n",
      "| epoch 323 |  iter 1 / 3 | time 0[s] | loss 0.48\n",
      "| epoch 324 |  iter 1 / 3 | time 0[s] | loss 0.47\n",
      "| epoch 325 |  iter 1 / 3 | time 0[s] | loss 0.47\n",
      "| epoch 326 |  iter 1 / 3 | time 0[s] | loss 0.40\n",
      "| epoch 327 |  iter 1 / 3 | time 0[s] | loss 0.47\n",
      "| epoch 328 |  iter 1 / 3 | time 0[s] | loss 0.52\n",
      "| epoch 329 |  iter 1 / 3 | time 0[s] | loss 0.41\n",
      "| epoch 330 |  iter 1 / 3 | time 0[s] | loss 0.53\n",
      "| epoch 331 |  iter 1 / 3 | time 0[s] | loss 0.44\n",
      "| epoch 332 |  iter 1 / 3 | time 0[s] | loss 0.45\n",
      "| epoch 333 |  iter 1 / 3 | time 0[s] | loss 0.40\n",
      "| epoch 334 |  iter 1 / 3 | time 0[s] | loss 0.45\n",
      "| epoch 335 |  iter 1 / 3 | time 0[s] | loss 0.44\n",
      "| epoch 336 |  iter 1 / 3 | time 0[s] | loss 0.51\n",
      "| epoch 337 |  iter 1 / 3 | time 0[s] | loss 0.37\n",
      "| epoch 338 |  iter 1 / 3 | time 0[s] | loss 0.49\n",
      "| epoch 339 |  iter 1 / 3 | time 0[s] | loss 0.45\n",
      "| epoch 340 |  iter 1 / 3 | time 0[s] | loss 0.41\n",
      "| epoch 341 |  iter 1 / 3 | time 0[s] | loss 0.38\n",
      "| epoch 342 |  iter 1 / 3 | time 0[s] | loss 0.49\n",
      "| epoch 343 |  iter 1 / 3 | time 0[s] | loss 0.42\n",
      "| epoch 344 |  iter 1 / 3 | time 0[s] | loss 0.40\n",
      "| epoch 345 |  iter 1 / 3 | time 0[s] | loss 0.37\n",
      "| epoch 346 |  iter 1 / 3 | time 0[s] | loss 0.48\n",
      "| epoch 347 |  iter 1 / 3 | time 0[s] | loss 0.41\n",
      "| epoch 348 |  iter 1 / 3 | time 0[s] | loss 0.41\n",
      "| epoch 349 |  iter 1 / 3 | time 0[s] | loss 0.39\n",
      "| epoch 350 |  iter 1 / 3 | time 0[s] | loss 0.41\n",
      "| epoch 351 |  iter 1 / 3 | time 0[s] | loss 0.42\n",
      "| epoch 352 |  iter 1 / 3 | time 0[s] | loss 0.34\n",
      "| epoch 353 |  iter 1 / 3 | time 0[s] | loss 0.40\n",
      "| epoch 354 |  iter 1 / 3 | time 0[s] | loss 0.46\n",
      "| epoch 355 |  iter 1 / 3 | time 0[s] | loss 0.33\n",
      "| epoch 356 |  iter 1 / 3 | time 0[s] | loss 0.45\n",
      "| epoch 357 |  iter 1 / 3 | time 0[s] | loss 0.39\n",
      "| epoch 358 |  iter 1 / 3 | time 0[s] | loss 0.37\n",
      "| epoch 359 |  iter 1 / 3 | time 0[s] | loss 0.34\n",
      "| epoch 360 |  iter 1 / 3 | time 0[s] | loss 0.44\n",
      "| epoch 361 |  iter 1 / 3 | time 0[s] | loss 0.36\n",
      "| epoch 362 |  iter 1 / 3 | time 0[s] | loss 0.39\n",
      "| epoch 363 |  iter 1 / 3 | time 0[s] | loss 0.31\n",
      "| epoch 364 |  iter 1 / 3 | time 0[s] | loss 0.42\n",
      "| epoch 365 |  iter 1 / 3 | time 0[s] | loss 0.37\n",
      "| epoch 366 |  iter 1 / 3 | time 0[s] | loss 0.37\n",
      "| epoch 367 |  iter 1 / 3 | time 0[s] | loss 0.38\n",
      "| epoch 368 |  iter 1 / 3 | time 0[s] | loss 0.35\n",
      "| epoch 369 |  iter 1 / 3 | time 0[s] | loss 0.32\n",
      "| epoch 370 |  iter 1 / 3 | time 0[s] | loss 0.40\n",
      "| epoch 371 |  iter 1 / 3 | time 0[s] | loss 0.31\n",
      "| epoch 372 |  iter 1 / 3 | time 0[s] | loss 0.40\n",
      "| epoch 373 |  iter 1 / 3 | time 0[s] | loss 0.31\n",
      "| epoch 374 |  iter 1 / 3 | time 0[s] | loss 0.35\n",
      "| epoch 375 |  iter 1 / 3 | time 0[s] | loss 0.40\n",
      "| epoch 376 |  iter 1 / 3 | time 0[s] | loss 0.33\n",
      "| epoch 377 |  iter 1 / 3 | time 0[s] | loss 0.30\n",
      "| epoch 378 |  iter 1 / 3 | time 0[s] | loss 0.34\n",
      "| epoch 379 |  iter 1 / 3 | time 0[s] | loss 0.40\n",
      "| epoch 380 |  iter 1 / 3 | time 0[s] | loss 0.33\n",
      "| epoch 381 |  iter 1 / 3 | time 0[s] | loss 0.34\n",
      "| epoch 382 |  iter 1 / 3 | time 0[s] | loss 0.35\n",
      "| epoch 383 |  iter 1 / 3 | time 0[s] | loss 0.28\n",
      "| epoch 384 |  iter 1 / 3 | time 0[s] | loss 0.38\n",
      "| epoch 385 |  iter 1 / 3 | time 0[s] | loss 0.28\n",
      "| epoch 386 |  iter 1 / 3 | time 0[s] | loss 0.38\n",
      "| epoch 387 |  iter 1 / 3 | time 0[s] | loss 0.31\n",
      "| epoch 388 |  iter 1 / 3 | time 0[s] | loss 0.28\n",
      "| epoch 389 |  iter 1 / 3 | time 0[s] | loss 0.36\n",
      "| epoch 390 |  iter 1 / 3 | time 0[s] | loss 0.32\n",
      "| epoch 391 |  iter 1 / 3 | time 0[s] | loss 0.32\n",
      "| epoch 392 |  iter 1 / 3 | time 0[s] | loss 0.33\n",
      "| epoch 393 |  iter 1 / 3 | time 0[s] | loss 0.30\n",
      "| epoch 394 |  iter 1 / 3 | time 0[s] | loss 0.31\n",
      "| epoch 395 |  iter 1 / 3 | time 0[s] | loss 0.31\n",
      "| epoch 396 |  iter 1 / 3 | time 0[s] | loss 0.27\n",
      "| epoch 397 |  iter 1 / 3 | time 0[s] | loss 0.35\n",
      "| epoch 398 |  iter 1 / 3 | time 0[s] | loss 0.30\n",
      "| epoch 399 |  iter 1 / 3 | time 0[s] | loss 0.25\n",
      "| epoch 400 |  iter 1 / 3 | time 0[s] | loss 0.34\n",
      "| epoch 401 |  iter 1 / 3 | time 0[s] | loss 0.30\n",
      "| epoch 402 |  iter 1 / 3 | time 0[s] | loss 0.31\n",
      "| epoch 403 |  iter 1 / 3 | time 0[s] | loss 0.25\n",
      "| epoch 404 |  iter 1 / 3 | time 0[s] | loss 0.33\n",
      "| epoch 405 |  iter 1 / 3 | time 0[s] | loss 0.26\n",
      "| epoch 406 |  iter 1 / 3 | time 0[s] | loss 0.33\n",
      "| epoch 407 |  iter 1 / 3 | time 0[s] | loss 0.30\n",
      "| epoch 408 |  iter 1 / 3 | time 0[s] | loss 0.24\n",
      "| epoch 409 |  iter 1 / 3 | time 0[s] | loss 0.32\n",
      "| epoch 410 |  iter 1 / 3 | time 0[s] | loss 0.28\n",
      "| epoch 411 |  iter 1 / 3 | time 0[s] | loss 0.28\n",
      "| epoch 412 |  iter 1 / 3 | time 0[s] | loss 0.28\n",
      "| epoch 413 |  iter 1 / 3 | time 0[s] | loss 0.28\n",
      "| epoch 414 |  iter 1 / 3 | time 0[s] | loss 0.28\n",
      "| epoch 415 |  iter 1 / 3 | time 0[s] | loss 0.24\n",
      "| epoch 416 |  iter 1 / 3 | time 0[s] | loss 0.27\n",
      "| epoch 417 |  iter 1 / 3 | time 0[s] | loss 0.27\n",
      "| epoch 418 |  iter 1 / 3 | time 0[s] | loss 0.27\n",
      "| epoch 419 |  iter 1 / 3 | time 0[s] | loss 0.30\n",
      "| epoch 420 |  iter 1 / 3 | time 0[s] | loss 0.27\n",
      "| epoch 421 |  iter 1 / 3 | time 0[s] | loss 0.23\n",
      "| epoch 422 |  iter 1 / 3 | time 0[s] | loss 0.31\n",
      "| epoch 423 |  iter 1 / 3 | time 0[s] | loss 0.22\n",
      "| epoch 424 |  iter 1 / 3 | time 0[s] | loss 0.29\n",
      "| epoch 425 |  iter 1 / 3 | time 0[s] | loss 0.26\n",
      "| epoch 426 |  iter 1 / 3 | time 0[s] | loss 0.27\n",
      "| epoch 427 |  iter 1 / 3 | time 0[s] | loss 0.22\n",
      "| epoch 428 |  iter 1 / 3 | time 0[s] | loss 0.29\n",
      "| epoch 429 |  iter 1 / 3 | time 0[s] | loss 0.22\n",
      "| epoch 430 |  iter 1 / 3 | time 0[s] | loss 0.29\n",
      "| epoch 431 |  iter 1 / 3 | time 0[s] | loss 0.21\n",
      "| epoch 432 |  iter 1 / 3 | time 0[s] | loss 0.29\n",
      "| epoch 433 |  iter 1 / 3 | time 0[s] | loss 0.25\n",
      "| epoch 434 |  iter 1 / 3 | time 0[s] | loss 0.24\n",
      "| epoch 435 |  iter 1 / 3 | time 0[s] | loss 0.25\n",
      "| epoch 436 |  iter 1 / 3 | time 0[s] | loss 0.24\n",
      "| epoch 437 |  iter 1 / 3 | time 0[s] | loss 0.24\n",
      "| epoch 438 |  iter 1 / 3 | time 0[s] | loss 0.21\n",
      "| epoch 439 |  iter 1 / 3 | time 0[s] | loss 0.27\n",
      "| epoch 440 |  iter 1 / 3 | time 0[s] | loss 0.21\n",
      "| epoch 441 |  iter 1 / 3 | time 0[s] | loss 0.24\n",
      "| epoch 442 |  iter 1 / 3 | time 0[s] | loss 0.27\n",
      "| epoch 443 |  iter 1 / 3 | time 0[s] | loss 0.23\n",
      "| epoch 444 |  iter 1 / 3 | time 0[s] | loss 0.20\n",
      "| epoch 445 |  iter 1 / 3 | time 0[s] | loss 0.26\n",
      "| epoch 446 |  iter 1 / 3 | time 0[s] | loss 0.20\n",
      "| epoch 447 |  iter 1 / 3 | time 0[s] | loss 0.26\n",
      "| epoch 448 |  iter 1 / 3 | time 0[s] | loss 0.24\n",
      "| epoch 449 |  iter 1 / 3 | time 0[s] | loss 0.22\n",
      "| epoch 450 |  iter 1 / 3 | time 0[s] | loss 0.20\n",
      "| epoch 451 |  iter 1 / 3 | time 0[s] | loss 0.25\n",
      "| epoch 452 |  iter 1 / 3 | time 0[s] | loss 0.23\n",
      "| epoch 453 |  iter 1 / 3 | time 0[s] | loss 0.22\n",
      "| epoch 454 |  iter 1 / 3 | time 0[s] | loss 0.22\n",
      "| epoch 455 |  iter 1 / 3 | time 0[s] | loss 0.19\n",
      "| epoch 456 |  iter 1 / 3 | time 0[s] | loss 0.22\n",
      "| epoch 457 |  iter 1 / 3 | time 0[s] | loss 0.22\n",
      "| epoch 458 |  iter 1 / 3 | time 0[s] | loss 0.24\n",
      "| epoch 459 |  iter 1 / 3 | time 0[s] | loss 0.19\n",
      "| epoch 460 |  iter 1 / 3 | time 0[s] | loss 0.21\n",
      "| epoch 461 |  iter 1 / 3 | time 0[s] | loss 0.21\n",
      "| epoch 462 |  iter 1 / 3 | time 0[s] | loss 0.24\n",
      "| epoch 463 |  iter 1 / 3 | time 0[s] | loss 0.18\n",
      "| epoch 464 |  iter 1 / 3 | time 0[s] | loss 0.21\n",
      "| epoch 465 |  iter 1 / 3 | time 0[s] | loss 0.21\n",
      "| epoch 466 |  iter 1 / 3 | time 0[s] | loss 0.23\n",
      "| epoch 467 |  iter 1 / 3 | time 0[s] | loss 0.21\n",
      "| epoch 468 |  iter 1 / 3 | time 0[s] | loss 0.20\n",
      "| epoch 469 |  iter 1 / 3 | time 0[s] | loss 0.20\n",
      "| epoch 470 |  iter 1 / 3 | time 0[s] | loss 0.21\n",
      "| epoch 471 |  iter 1 / 3 | time 0[s] | loss 0.19\n",
      "| epoch 472 |  iter 1 / 3 | time 0[s] | loss 0.21\n",
      "| epoch 473 |  iter 1 / 3 | time 0[s] | loss 0.20\n",
      "| epoch 474 |  iter 1 / 3 | time 0[s] | loss 0.17\n",
      "| epoch 475 |  iter 1 / 3 | time 0[s] | loss 0.23\n",
      "| epoch 476 |  iter 1 / 3 | time 0[s] | loss 0.19\n",
      "| epoch 477 |  iter 1 / 3 | time 0[s] | loss 0.17\n",
      "| epoch 478 |  iter 1 / 3 | time 0[s] | loss 0.22\n",
      "| epoch 479 |  iter 1 / 3 | time 0[s] | loss 0.20\n",
      "| epoch 480 |  iter 1 / 3 | time 0[s] | loss 0.19\n",
      "| epoch 481 |  iter 1 / 3 | time 0[s] | loss 0.16\n",
      "| epoch 482 |  iter 1 / 3 | time 0[s] | loss 0.22\n",
      "| epoch 483 |  iter 1 / 3 | time 0[s] | loss 0.16\n",
      "| epoch 484 |  iter 1 / 3 | time 0[s] | loss 0.19\n",
      "| epoch 485 |  iter 1 / 3 | time 0[s] | loss 0.19\n",
      "| epoch 486 |  iter 1 / 3 | time 0[s] | loss 0.21\n",
      "| epoch 487 |  iter 1 / 3 | time 0[s] | loss 0.18\n",
      "| epoch 488 |  iter 1 / 3 | time 0[s] | loss 0.18\n",
      "| epoch 489 |  iter 1 / 3 | time 0[s] | loss 0.15\n",
      "| epoch 490 |  iter 1 / 3 | time 0[s] | loss 0.21\n",
      "| epoch 491 |  iter 1 / 3 | time 0[s] | loss 0.18\n",
      "| epoch 492 |  iter 1 / 3 | time 0[s] | loss 0.15\n",
      "| epoch 493 |  iter 1 / 3 | time 0[s] | loss 0.20\n",
      "| epoch 494 |  iter 1 / 3 | time 0[s] | loss 0.18\n",
      "| epoch 495 |  iter 1 / 3 | time 0[s] | loss 0.17\n",
      "| epoch 496 |  iter 1 / 3 | time 0[s] | loss 0.15\n",
      "| epoch 497 |  iter 1 / 3 | time 0[s] | loss 0.20\n",
      "| epoch 498 |  iter 1 / 3 | time 0[s] | loss 0.18\n",
      "| epoch 499 |  iter 1 / 3 | time 0[s] | loss 0.17\n",
      "| epoch 500 |  iter 1 / 3 | time 0[s] | loss 0.15\n",
      "| epoch 501 |  iter 1 / 3 | time 0[s] | loss 0.19\n",
      "| epoch 502 |  iter 1 / 3 | time 0[s] | loss 0.17\n",
      "| epoch 503 |  iter 1 / 3 | time 0[s] | loss 0.17\n",
      "| epoch 504 |  iter 1 / 3 | time 0[s] | loss 0.17\n",
      "| epoch 505 |  iter 1 / 3 | time 0[s] | loss 0.17\n",
      "| epoch 506 |  iter 1 / 3 | time 0[s] | loss 0.16\n",
      "| epoch 507 |  iter 1 / 3 | time 0[s] | loss 0.15\n",
      "| epoch 508 |  iter 1 / 3 | time 0[s] | loss 0.16\n",
      "| epoch 509 |  iter 1 / 3 | time 0[s] | loss 0.19\n",
      "| epoch 510 |  iter 1 / 3 | time 0[s] | loss 0.16\n",
      "| epoch 511 |  iter 1 / 3 | time 0[s] | loss 0.17\n",
      "| epoch 512 |  iter 1 / 3 | time 0[s] | loss 0.16\n",
      "| epoch 513 |  iter 1 / 3 | time 0[s] | loss 0.16\n",
      "| epoch 514 |  iter 1 / 3 | time 0[s] | loss 0.16\n",
      "| epoch 515 |  iter 1 / 3 | time 0[s] | loss 0.14\n",
      "| epoch 516 |  iter 1 / 3 | time 0[s] | loss 0.18\n",
      "| epoch 517 |  iter 1 / 3 | time 0[s] | loss 0.15\n",
      "| epoch 518 |  iter 1 / 3 | time 0[s] | loss 0.14\n",
      "| epoch 519 |  iter 1 / 3 | time 0[s] | loss 0.17\n",
      "| epoch 520 |  iter 1 / 3 | time 0[s] | loss 0.15\n",
      "| epoch 521 |  iter 1 / 3 | time 0[s] | loss 0.14\n",
      "| epoch 522 |  iter 1 / 3 | time 0[s] | loss 0.15\n",
      "| epoch 523 |  iter 1 / 3 | time 0[s] | loss 0.15\n",
      "| epoch 524 |  iter 1 / 3 | time 0[s] | loss 0.17\n",
      "| epoch 525 |  iter 1 / 3 | time 0[s] | loss 0.13\n",
      "| epoch 526 |  iter 1 / 3 | time 0[s] | loss 0.17\n",
      "| epoch 527 |  iter 1 / 3 | time 0[s] | loss 0.15\n",
      "| epoch 528 |  iter 1 / 3 | time 0[s] | loss 0.13\n",
      "| epoch 529 |  iter 1 / 3 | time 0[s] | loss 0.15\n",
      "| epoch 530 |  iter 1 / 3 | time 0[s] | loss 0.15\n",
      "| epoch 531 |  iter 1 / 3 | time 0[s] | loss 0.15\n",
      "| epoch 532 |  iter 1 / 3 | time 0[s] | loss 0.17\n",
      "| epoch 533 |  iter 1 / 3 | time 0[s] | loss 0.14\n",
      "| epoch 534 |  iter 1 / 3 | time 0[s] | loss 0.14\n",
      "| epoch 535 |  iter 1 / 3 | time 0[s] | loss 0.15\n",
      "| epoch 536 |  iter 1 / 3 | time 0[s] | loss 0.12\n",
      "| epoch 537 |  iter 1 / 3 | time 0[s] | loss 0.16\n",
      "| epoch 538 |  iter 1 / 3 | time 0[s] | loss 0.14\n",
      "| epoch 539 |  iter 1 / 3 | time 0[s] | loss 0.14\n",
      "| epoch 540 |  iter 1 / 3 | time 0[s] | loss 0.12\n",
      "| epoch 541 |  iter 1 / 3 | time 0[s] | loss 0.16\n",
      "| epoch 542 |  iter 1 / 3 | time 0[s] | loss 0.12\n",
      "| epoch 543 |  iter 1 / 3 | time 0[s] | loss 0.15\n",
      "| epoch 544 |  iter 1 / 3 | time 0[s] | loss 0.12\n",
      "| epoch 545 |  iter 1 / 3 | time 0[s] | loss 0.16\n",
      "| epoch 546 |  iter 1 / 3 | time 0[s] | loss 0.13\n",
      "| epoch 547 |  iter 1 / 3 | time 0[s] | loss 0.12\n",
      "| epoch 548 |  iter 1 / 3 | time 0[s] | loss 0.15\n",
      "| epoch 549 |  iter 1 / 3 | time 0[s] | loss 0.13\n",
      "| epoch 550 |  iter 1 / 3 | time 0[s] | loss 0.14\n",
      "| epoch 551 |  iter 1 / 3 | time 0[s] | loss 0.13\n",
      "| epoch 552 |  iter 1 / 3 | time 0[s] | loss 0.11\n",
      "| epoch 553 |  iter 1 / 3 | time 0[s] | loss 0.13\n",
      "| epoch 554 |  iter 1 / 3 | time 0[s] | loss 0.15\n",
      "| epoch 555 |  iter 1 / 3 | time 0[s] | loss 0.11\n",
      "| epoch 556 |  iter 1 / 3 | time 0[s] | loss 0.15\n",
      "| epoch 557 |  iter 1 / 3 | time 0[s] | loss 0.13\n",
      "| epoch 558 |  iter 1 / 3 | time 0[s] | loss 0.13\n",
      "| epoch 559 |  iter 1 / 3 | time 0[s] | loss 0.11\n",
      "| epoch 560 |  iter 1 / 3 | time 0[s] | loss 0.13\n",
      "| epoch 561 |  iter 1 / 3 | time 0[s] | loss 0.14\n",
      "| epoch 562 |  iter 1 / 3 | time 0[s] | loss 0.13\n",
      "| epoch 563 |  iter 1 / 3 | time 0[s] | loss 0.11\n",
      "| epoch 564 |  iter 1 / 3 | time 0[s] | loss 0.12\n",
      "| epoch 565 |  iter 1 / 3 | time 0[s] | loss 0.12\n",
      "| epoch 566 |  iter 1 / 3 | time 0[s] | loss 0.14\n",
      "| epoch 567 |  iter 1 / 3 | time 0[s] | loss 0.12\n",
      "| epoch 568 |  iter 1 / 3 | time 0[s] | loss 0.11\n",
      "| epoch 569 |  iter 1 / 3 | time 0[s] | loss 0.14\n",
      "| epoch 570 |  iter 1 / 3 | time 0[s] | loss 0.12\n",
      "| epoch 571 |  iter 1 / 3 | time 0[s] | loss 0.12\n",
      "| epoch 572 |  iter 1 / 3 | time 0[s] | loss 0.11\n",
      "| epoch 573 |  iter 1 / 3 | time 0[s] | loss 0.14\n",
      "| epoch 574 |  iter 1 / 3 | time 0[s] | loss 0.12\n",
      "| epoch 575 |  iter 1 / 3 | time 0[s] | loss 0.12\n",
      "| epoch 576 |  iter 1 / 3 | time 0[s] | loss 0.12\n",
      "| epoch 577 |  iter 1 / 3 | time 0[s] | loss 0.11\n",
      "| epoch 578 |  iter 1 / 3 | time 0[s] | loss 0.10\n",
      "| epoch 579 |  iter 1 / 3 | time 0[s] | loss 0.13\n",
      "| epoch 580 |  iter 1 / 3 | time 0[s] | loss 0.10\n",
      "| epoch 581 |  iter 1 / 3 | time 0[s] | loss 0.11\n",
      "| epoch 582 |  iter 1 / 3 | time 0[s] | loss 0.13\n",
      "| epoch 583 |  iter 1 / 3 | time 0[s] | loss 0.10\n",
      "| epoch 584 |  iter 1 / 3 | time 0[s] | loss 0.11\n",
      "| epoch 585 |  iter 1 / 3 | time 0[s] | loss 0.13\n",
      "| epoch 586 |  iter 1 / 3 | time 0[s] | loss 0.10\n",
      "| epoch 587 |  iter 1 / 3 | time 0[s] | loss 0.12\n",
      "| epoch 588 |  iter 1 / 3 | time 0[s] | loss 0.11\n",
      "| epoch 589 |  iter 1 / 3 | time 0[s] | loss 0.11\n",
      "| epoch 590 |  iter 1 / 3 | time 0[s] | loss 0.11\n",
      "| epoch 591 |  iter 1 / 3 | time 0[s] | loss 0.09\n",
      "| epoch 592 |  iter 1 / 3 | time 0[s] | loss 0.12\n",
      "| epoch 593 |  iter 1 / 3 | time 0[s] | loss 0.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 594 |  iter 1 / 3 | time 0[s] | loss 0.11\n",
      "| epoch 595 |  iter 1 / 3 | time 0[s] | loss 0.11\n",
      "| epoch 596 |  iter 1 / 3 | time 0[s] | loss 0.11\n",
      "| epoch 597 |  iter 1 / 3 | time 0[s] | loss 0.10\n",
      "| epoch 598 |  iter 1 / 3 | time 0[s] | loss 0.11\n",
      "| epoch 599 |  iter 1 / 3 | time 0[s] | loss 0.10\n",
      "| epoch 600 |  iter 1 / 3 | time 0[s] | loss 0.09\n",
      "| epoch 601 |  iter 1 / 3 | time 0[s] | loss 0.12\n",
      "| epoch 602 |  iter 1 / 3 | time 0[s] | loss 0.10\n",
      "| epoch 603 |  iter 1 / 3 | time 0[s] | loss 0.09\n",
      "| epoch 604 |  iter 1 / 3 | time 0[s] | loss 0.10\n",
      "| epoch 605 |  iter 1 / 3 | time 0[s] | loss 0.11\n",
      "| epoch 606 |  iter 1 / 3 | time 0[s] | loss 0.10\n",
      "| epoch 607 |  iter 1 / 3 | time 0[s] | loss 0.09\n",
      "| epoch 608 |  iter 1 / 3 | time 0[s] | loss 0.11\n",
      "| epoch 609 |  iter 1 / 3 | time 0[s] | loss 0.10\n",
      "| epoch 610 |  iter 1 / 3 | time 0[s] | loss 0.10\n",
      "| epoch 611 |  iter 1 / 3 | time 0[s] | loss 0.10\n",
      "| epoch 612 |  iter 1 / 3 | time 0[s] | loss 0.09\n",
      "| epoch 613 |  iter 1 / 3 | time 0[s] | loss 0.10\n",
      "| epoch 614 |  iter 1 / 3 | time 0[s] | loss 0.11\n",
      "| epoch 615 |  iter 1 / 3 | time 0[s] | loss 0.10\n",
      "| epoch 616 |  iter 1 / 3 | time 0[s] | loss 0.09\n",
      "| epoch 617 |  iter 1 / 3 | time 0[s] | loss 0.10\n",
      "| epoch 618 |  iter 1 / 3 | time 0[s] | loss 0.11\n",
      "| epoch 619 |  iter 1 / 3 | time 0[s] | loss 0.08\n",
      "| epoch 620 |  iter 1 / 3 | time 0[s] | loss 0.10\n",
      "| epoch 621 |  iter 1 / 3 | time 0[s] | loss 0.10\n",
      "| epoch 622 |  iter 1 / 3 | time 0[s] | loss 0.10\n",
      "| epoch 623 |  iter 1 / 3 | time 0[s] | loss 0.11\n",
      "| epoch 624 |  iter 1 / 3 | time 0[s] | loss 0.10\n",
      "| epoch 625 |  iter 1 / 3 | time 0[s] | loss 0.09\n",
      "| epoch 626 |  iter 1 / 3 | time 0[s] | loss 0.09\n",
      "| epoch 627 |  iter 1 / 3 | time 0[s] | loss 0.08\n",
      "| epoch 628 |  iter 1 / 3 | time 0[s] | loss 0.09\n",
      "| epoch 629 |  iter 1 / 3 | time 0[s] | loss 0.10\n",
      "| epoch 630 |  iter 1 / 3 | time 0[s] | loss 0.09\n",
      "| epoch 631 |  iter 1 / 3 | time 0[s] | loss 0.09\n",
      "| epoch 632 |  iter 1 / 3 | time 0[s] | loss 0.09\n",
      "| epoch 633 |  iter 1 / 3 | time 0[s] | loss 0.08\n",
      "| epoch 634 |  iter 1 / 3 | time 0[s] | loss 0.10\n",
      "| epoch 635 |  iter 1 / 3 | time 0[s] | loss 0.09\n",
      "| epoch 636 |  iter 1 / 3 | time 0[s] | loss 0.09\n",
      "| epoch 637 |  iter 1 / 3 | time 0[s] | loss 0.08\n",
      "| epoch 638 |  iter 1 / 3 | time 0[s] | loss 0.10\n",
      "| epoch 639 |  iter 1 / 3 | time 0[s] | loss 0.09\n",
      "| epoch 640 |  iter 1 / 3 | time 0[s] | loss 0.09\n",
      "| epoch 641 |  iter 1 / 3 | time 0[s] | loss 0.09\n",
      "| epoch 642 |  iter 1 / 3 | time 0[s] | loss 0.09\n",
      "| epoch 643 |  iter 1 / 3 | time 0[s] | loss 0.09\n",
      "| epoch 644 |  iter 1 / 3 | time 0[s] | loss 0.08\n",
      "| epoch 645 |  iter 1 / 3 | time 0[s] | loss 0.09\n",
      "| epoch 646 |  iter 1 / 3 | time 0[s] | loss 0.09\n",
      "| epoch 647 |  iter 1 / 3 | time 0[s] | loss 0.10\n",
      "| epoch 648 |  iter 1 / 3 | time 0[s] | loss 0.08\n",
      "| epoch 649 |  iter 1 / 3 | time 0[s] | loss 0.08\n",
      "| epoch 650 |  iter 1 / 3 | time 0[s] | loss 0.07\n",
      "| epoch 651 |  iter 1 / 3 | time 0[s] | loss 0.09\n",
      "| epoch 652 |  iter 1 / 3 | time 0[s] | loss 0.08\n",
      "| epoch 653 |  iter 1 / 3 | time 0[s] | loss 0.08\n",
      "| epoch 654 |  iter 1 / 3 | time 0[s] | loss 0.07\n",
      "| epoch 655 |  iter 1 / 3 | time 0[s] | loss 0.08\n",
      "| epoch 656 |  iter 1 / 3 | time 0[s] | loss 0.09\n",
      "| epoch 657 |  iter 1 / 3 | time 0[s] | loss 0.08\n",
      "| epoch 658 |  iter 1 / 3 | time 0[s] | loss 0.08\n",
      "| epoch 659 |  iter 1 / 3 | time 0[s] | loss 0.08\n",
      "| epoch 660 |  iter 1 / 3 | time 0[s] | loss 0.08\n",
      "| epoch 661 |  iter 1 / 3 | time 0[s] | loss 0.08\n",
      "| epoch 662 |  iter 1 / 3 | time 0[s] | loss 0.08\n",
      "| epoch 663 |  iter 1 / 3 | time 0[s] | loss 0.08\n",
      "| epoch 664 |  iter 1 / 3 | time 0[s] | loss 0.08\n",
      "| epoch 665 |  iter 1 / 3 | time 0[s] | loss 0.08\n",
      "| epoch 666 |  iter 1 / 3 | time 0[s] | loss 0.07\n",
      "| epoch 667 |  iter 1 / 3 | time 0[s] | loss 0.09\n",
      "| epoch 668 |  iter 1 / 3 | time 0[s] | loss 0.08\n",
      "| epoch 669 |  iter 1 / 3 | time 0[s] | loss 0.08\n",
      "| epoch 670 |  iter 1 / 3 | time 0[s] | loss 0.08\n",
      "| epoch 671 |  iter 1 / 3 | time 0[s] | loss 0.08\n",
      "| epoch 672 |  iter 1 / 3 | time 0[s] | loss 0.07\n",
      "| epoch 673 |  iter 1 / 3 | time 0[s] | loss 0.08\n",
      "| epoch 674 |  iter 1 / 3 | time 0[s] | loss 0.08\n",
      "| epoch 675 |  iter 1 / 3 | time 0[s] | loss 0.07\n",
      "| epoch 676 |  iter 1 / 3 | time 0[s] | loss 0.07\n",
      "| epoch 677 |  iter 1 / 3 | time 0[s] | loss 0.08\n",
      "| epoch 678 |  iter 1 / 3 | time 0[s] | loss 0.08\n",
      "| epoch 679 |  iter 1 / 3 | time 0[s] | loss 0.07\n",
      "| epoch 680 |  iter 1 / 3 | time 0[s] | loss 0.07\n",
      "| epoch 681 |  iter 1 / 3 | time 0[s] | loss 0.08\n",
      "| epoch 682 |  iter 1 / 3 | time 0[s] | loss 0.07\n",
      "| epoch 683 |  iter 1 / 3 | time 0[s] | loss 0.07\n",
      "| epoch 684 |  iter 1 / 3 | time 0[s] | loss 0.08\n",
      "| epoch 685 |  iter 1 / 3 | time 0[s] | loss 0.07\n",
      "| epoch 686 |  iter 1 / 3 | time 0[s] | loss 0.07\n",
      "| epoch 687 |  iter 1 / 3 | time 0[s] | loss 0.07\n",
      "| epoch 688 |  iter 1 / 3 | time 0[s] | loss 0.07\n",
      "| epoch 689 |  iter 1 / 3 | time 0[s] | loss 0.07\n",
      "| epoch 690 |  iter 1 / 3 | time 0[s] | loss 0.07\n",
      "| epoch 691 |  iter 1 / 3 | time 0[s] | loss 0.06\n",
      "| epoch 692 |  iter 1 / 3 | time 0[s] | loss 0.08\n",
      "| epoch 693 |  iter 1 / 3 | time 0[s] | loss 0.06\n",
      "| epoch 694 |  iter 1 / 3 | time 0[s] | loss 0.08\n",
      "| epoch 695 |  iter 1 / 3 | time 0[s] | loss 0.07\n",
      "| epoch 696 |  iter 1 / 3 | time 0[s] | loss 0.07\n",
      "| epoch 697 |  iter 1 / 3 | time 0[s] | loss 0.07\n",
      "| epoch 698 |  iter 1 / 3 | time 0[s] | loss 0.07\n",
      "| epoch 699 |  iter 1 / 3 | time 0[s] | loss 0.06\n",
      "| epoch 700 |  iter 1 / 3 | time 0[s] | loss 0.08\n",
      "| epoch 701 |  iter 1 / 3 | time 0[s] | loss 0.07\n",
      "| epoch 702 |  iter 1 / 3 | time 0[s] | loss 0.07\n",
      "| epoch 703 |  iter 1 / 3 | time 0[s] | loss 0.07\n",
      "| epoch 704 |  iter 1 / 3 | time 0[s] | loss 0.07\n",
      "| epoch 705 |  iter 1 / 3 | time 0[s] | loss 0.06\n",
      "| epoch 706 |  iter 1 / 3 | time 0[s] | loss 0.07\n",
      "| epoch 707 |  iter 1 / 3 | time 0[s] | loss 0.07\n",
      "| epoch 708 |  iter 1 / 3 | time 0[s] | loss 0.07\n",
      "| epoch 709 |  iter 1 / 3 | time 0[s] | loss 0.06\n",
      "| epoch 710 |  iter 1 / 3 | time 0[s] | loss 0.07\n",
      "| epoch 711 |  iter 1 / 3 | time 0[s] | loss 0.07\n",
      "| epoch 712 |  iter 1 / 3 | time 0[s] | loss 0.07\n",
      "| epoch 713 |  iter 1 / 3 | time 0[s] | loss 0.07\n",
      "| epoch 714 |  iter 1 / 3 | time 0[s] | loss 0.06\n",
      "| epoch 715 |  iter 1 / 3 | time 0[s] | loss 0.07\n",
      "| epoch 716 |  iter 1 / 3 | time 0[s] | loss 0.06\n",
      "| epoch 717 |  iter 1 / 3 | time 0[s] | loss 0.06\n",
      "| epoch 718 |  iter 1 / 3 | time 0[s] | loss 0.07\n",
      "| epoch 719 |  iter 1 / 3 | time 0[s] | loss 0.06\n",
      "| epoch 720 |  iter 1 / 3 | time 0[s] | loss 0.06\n",
      "| epoch 721 |  iter 1 / 3 | time 0[s] | loss 0.06\n",
      "| epoch 722 |  iter 1 / 3 | time 0[s] | loss 0.06\n",
      "| epoch 723 |  iter 1 / 3 | time 0[s] | loss 0.07\n",
      "| epoch 724 |  iter 1 / 3 | time 0[s] | loss 0.06\n",
      "| epoch 725 |  iter 1 / 3 | time 0[s] | loss 0.06\n",
      "| epoch 726 |  iter 1 / 3 | time 0[s] | loss 0.06\n",
      "| epoch 727 |  iter 1 / 3 | time 0[s] | loss 0.06\n",
      "| epoch 728 |  iter 1 / 3 | time 0[s] | loss 0.06\n",
      "| epoch 729 |  iter 1 / 3 | time 0[s] | loss 0.07\n",
      "| epoch 730 |  iter 1 / 3 | time 0[s] | loss 0.06\n",
      "| epoch 731 |  iter 1 / 3 | time 0[s] | loss 0.06\n",
      "| epoch 732 |  iter 1 / 3 | time 0[s] | loss 0.06\n",
      "| epoch 733 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 734 |  iter 1 / 3 | time 0[s] | loss 0.07\n",
      "| epoch 735 |  iter 1 / 3 | time 0[s] | loss 0.06\n",
      "| epoch 736 |  iter 1 / 3 | time 0[s] | loss 0.06\n",
      "| epoch 737 |  iter 1 / 3 | time 0[s] | loss 0.06\n",
      "| epoch 738 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 739 |  iter 1 / 3 | time 0[s] | loss 0.06\n",
      "| epoch 740 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 741 |  iter 1 / 3 | time 0[s] | loss 0.07\n",
      "| epoch 742 |  iter 1 / 3 | time 0[s] | loss 0.06\n",
      "| epoch 743 |  iter 1 / 3 | time 0[s] | loss 0.06\n",
      "| epoch 744 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 745 |  iter 1 / 3 | time 0[s] | loss 0.06\n",
      "| epoch 746 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 747 |  iter 1 / 3 | time 0[s] | loss 0.06\n",
      "| epoch 748 |  iter 1 / 3 | time 0[s] | loss 0.06\n",
      "| epoch 749 |  iter 1 / 3 | time 0[s] | loss 0.06\n",
      "| epoch 750 |  iter 1 / 3 | time 0[s] | loss 0.06\n",
      "| epoch 751 |  iter 1 / 3 | time 0[s] | loss 0.06\n",
      "| epoch 752 |  iter 1 / 3 | time 0[s] | loss 0.06\n",
      "| epoch 753 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 754 |  iter 1 / 3 | time 0[s] | loss 0.06\n",
      "| epoch 755 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 756 |  iter 1 / 3 | time 0[s] | loss 0.06\n",
      "| epoch 757 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 758 |  iter 1 / 3 | time 0[s] | loss 0.06\n",
      "| epoch 759 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 760 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 761 |  iter 1 / 3 | time 0[s] | loss 0.06\n",
      "| epoch 762 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 763 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 764 |  iter 1 / 3 | time 0[s] | loss 0.06\n",
      "| epoch 765 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 766 |  iter 1 / 3 | time 0[s] | loss 0.06\n",
      "| epoch 767 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 768 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 769 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 770 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 771 |  iter 1 / 3 | time 0[s] | loss 0.06\n",
      "| epoch 772 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 773 |  iter 1 / 3 | time 0[s] | loss 0.06\n",
      "| epoch 774 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 775 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 776 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 777 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 778 |  iter 1 / 3 | time 0[s] | loss 0.06\n",
      "| epoch 779 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 780 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 781 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 782 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 783 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 784 |  iter 1 / 3 | time 0[s] | loss 0.06\n",
      "| epoch 785 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 786 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 787 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 788 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 789 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 790 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 791 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 792 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 793 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 794 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 795 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 796 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 797 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 798 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 799 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 800 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 801 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 802 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 803 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 804 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 805 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 806 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 807 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 808 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 809 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 810 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 811 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 812 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 813 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 814 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 815 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 816 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 817 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 818 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 819 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 820 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 821 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 822 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 823 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 824 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 825 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 826 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 827 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 828 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 829 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 830 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 831 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 832 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 833 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 834 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 835 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 836 |  iter 1 / 3 | time 0[s] | loss 0.05\n",
      "| epoch 837 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 838 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 839 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 840 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 841 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 842 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 843 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 844 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 845 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 846 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 847 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 848 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 849 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 850 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 851 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 852 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 853 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 854 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 855 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 856 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 857 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 858 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 859 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 860 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 861 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 862 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 863 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 864 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 865 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 866 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 867 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 868 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 869 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 870 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 871 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 872 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 873 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 874 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 875 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 876 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 877 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 878 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 879 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 880 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 881 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 882 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 883 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 884 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 885 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 886 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 887 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 888 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 889 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 890 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 891 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 892 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 893 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 894 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 895 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 896 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 897 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 898 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 899 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 900 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 901 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 902 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 903 |  iter 1 / 3 | time 0[s] | loss 0.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 904 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 905 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 906 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 907 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 908 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 909 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 910 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 911 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 912 |  iter 1 / 3 | time 0[s] | loss 0.04\n",
      "| epoch 913 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 914 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 915 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 916 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 917 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 918 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 919 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 920 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 921 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 922 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 923 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 924 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 925 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 926 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 927 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 928 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 929 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 930 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 931 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 932 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 933 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 934 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 935 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 936 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 937 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 938 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 939 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 940 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 941 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 942 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 943 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 944 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 945 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 946 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 947 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 948 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 949 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 950 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 951 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 952 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 953 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 954 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 955 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 956 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 957 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 958 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 959 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 960 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 961 |  iter 1 / 3 | time 0[s] | loss 0.02\n",
      "| epoch 962 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 963 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 964 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 965 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 966 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 967 |  iter 1 / 3 | time 0[s] | loss 0.02\n",
      "| epoch 968 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 969 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 970 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 971 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 972 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 973 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 974 |  iter 1 / 3 | time 0[s] | loss 0.02\n",
      "| epoch 975 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 976 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 977 |  iter 1 / 3 | time 0[s] | loss 0.02\n",
      "| epoch 978 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 979 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 980 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 981 |  iter 1 / 3 | time 0[s] | loss 0.02\n",
      "| epoch 982 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 983 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 984 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 985 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 986 |  iter 1 / 3 | time 0[s] | loss 0.02\n",
      "| epoch 987 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 988 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 989 |  iter 1 / 3 | time 0[s] | loss 0.02\n",
      "| epoch 990 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 991 |  iter 1 / 3 | time 0[s] | loss 0.02\n",
      "| epoch 992 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 993 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 994 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 995 |  iter 1 / 3 | time 0[s] | loss 0.02\n",
      "| epoch 996 |  iter 1 / 3 | time 0[s] | loss 0.02\n",
      "| epoch 997 |  iter 1 / 3 | time 0[s] | loss 0.02\n",
      "| epoch 998 |  iter 1 / 3 | time 0[s] | loss 0.02\n",
      "| epoch 999 |  iter 1 / 3 | time 0[s] | loss 0.03\n",
      "| epoch 1000 |  iter 1 / 3 | time 0[s] | loss 0.02\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 模型训练 \n",
    "hidden_size = 3\n",
    "batch_size = 1\n",
    "max_epoch = 1000\n",
    "model = SimpleCBOW(vocab_size, hidden_size)   #SimpleCBOW模型\n",
    "optimizer = Adam()                          #优化器设置为Adam\n",
    "trainer = Trainer(model, optimizer)      #模型训练主体\n",
    "\n",
    "trainer.fit(contexts, target, max_epoch, batch_size)   #开始训练\n",
    "trainer.plot()   #画出loss图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我 [-1.475043   1.4098275 -1.462503 ]\n",
      "爱 [-1.1512601 -1.3358028 -1.1459126]\n",
      "你 [0.18974927 1.8748579  0.14052632]\n",
      "中 [-1.1669254 -1.3430164 -1.139884 ]\n",
      "国 [1.4321997 1.2569406 1.4441128]\n"
     ]
    }
   ],
   "source": [
    "# 得到词嵌入向量 \n",
    "word_vecs = model.word_vecs #保存的地方\n",
    "for word_id, word in id_to_word.items():\n",
    "    print(word, word_vecs[word_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**我们得到了词嵌入向量，当然，这里使用的一句话语料库太小了，换成更大的语料库，会获得更好的结果。在实际应用中，我们通常会直接方便快速的调用已经训练好的词嵌入模型来完成我们的任务**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2).调用word2vec模型完成词嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "import jieba\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**读取金融词汇txt文件并分词**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['涨停', '股', '', '', '免费', '下载', '', '', '证券时报', '网', '03', '月', '03', '日讯', '']\n"
     ]
    }
   ],
   "source": [
    "with open(\"EE.txt\", \"r\",encoding='utf-8') as f:  # 打开文件\n",
    "    data = f.read()  # 读取文件\n",
    "    #print(data)\n",
    "word_list = jieba.cut(data)\n",
    "result = ' '.join(word_list)\n",
    "result_s = result.split(' ')\n",
    "result_s = [result_s]\n",
    "print(result_s[0][5:20])    #这里没有去除标点符号等停用词，达到更好效果可参照第二章jieba分词来去除停用词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**调用并训练gensim自带的word2vec模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-28 03:11:11,140 : INFO : collecting all words and their counts\n",
      "2022-08-28 03:11:11,140 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2022-08-28 03:11:11,147 : INFO : collected 5126 word types from a corpus of 31069 raw words and 1 sentences\n",
      "2022-08-28 03:11:11,148 : INFO : Creating a fresh vocabulary\n",
      "2022-08-28 03:11:11,169 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 5126 unique words (100.00% of original 5126, drops 0)', 'datetime': '2022-08-28T03:11:11.169045', 'gensim': '4.2.0', 'python': '3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "2022-08-28 03:11:11,170 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 31069 word corpus (100.00% of original 31069, drops 0)', 'datetime': '2022-08-28T03:11:11.170046', 'gensim': '4.2.0', 'python': '3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "2022-08-28 03:11:11,205 : INFO : deleting the raw counts dictionary of 5126 items\n",
      "2022-08-28 03:11:11,206 : INFO : sample=0.001 downsamples 40 most-common words\n",
      "2022-08-28 03:11:11,206 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 26066.130454580634 word corpus (83.9%% of prior 31069)', 'datetime': '2022-08-28T03:11:11.206080', 'gensim': '4.2.0', 'python': '3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "2022-08-28 03:11:11,265 : INFO : estimated required memory for 5126 words and 100 dimensions: 6663800 bytes\n",
      "2022-08-28 03:11:11,265 : INFO : resetting layer weights\n",
      "2022-08-28 03:11:11,268 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-08-28T03:11:11.268444', 'gensim': '4.2.0', 'python': '3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'build_vocab'}\n",
      "2022-08-28 03:11:11,269 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 5126 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2022-08-28T03:11:11.269445', 'gensim': '4.2.0', 'python': '3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n",
      "2022-08-28 03:11:11,290 : INFO : EPOCH 0: training on 31069 raw words (10000 effective words) took 0.0s, 517333 effective words/s\n",
      "2022-08-28 03:11:11,307 : INFO : EPOCH 1: training on 31069 raw words (10000 effective words) took 0.0s, 668467 effective words/s\n",
      "2022-08-28 03:11:11,324 : INFO : EPOCH 2: training on 31069 raw words (10000 effective words) took 0.0s, 672332 effective words/s\n",
      "2022-08-28 03:11:11,341 : INFO : EPOCH 3: training on 31069 raw words (10000 effective words) took 0.0s, 658410 effective words/s\n",
      "2022-08-28 03:11:11,358 : INFO : EPOCH 4: training on 31069 raw words (10000 effective words) took 0.0s, 635898 effective words/s\n",
      "2022-08-28 03:11:11,359 : INFO : Word2Vec lifecycle event {'msg': 'training on 155345 raw words (50000 effective words) took 0.1s, 553176 effective words/s', 'datetime': '2022-08-28T03:11:11.359530', 'gensim': '4.2.0', 'python': '3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n",
      "2022-08-28 03:11:11,360 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=5126, vector_size=100, alpha=0.025>', 'datetime': '2022-08-28T03:11:11.360531', 'gensim': '4.2.0', 'python': '3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(result_s, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "涨停的词向量: [ 0.00022708  0.01069314 -0.00287368 -0.00671757  0.00764089 -0.00031531\n",
      "  0.01029458  0.01083129 -0.01368472  0.00317339 -0.00060533  0.00335896\n",
      "  0.0057543   0.00392208 -0.00396685  0.00044373  0.00832523  0.0119933\n",
      "  0.00302975  0.00085786  0.00201147  0.00549124  0.0112002  -0.01024245\n",
      " -0.00375853 -0.0057714   0.00407451 -0.0044203  -0.00936153 -0.00656092\n",
      "  0.00082509 -0.00287329 -0.01142543 -0.00212646  0.00931283 -0.00225077\n",
      "  0.00187799 -0.00075234 -0.00456681 -0.0024414  -0.00362836 -0.00314807\n",
      "  0.00133562 -0.01104223  0.00052073  0.00068026 -0.00823916  0.00681541\n",
      " -0.00158007  0.00266822  0.00145125 -0.00193912 -0.01452059 -0.00507084\n",
      " -0.00945021 -0.00119435  0.00375323  0.0072699   0.00250392 -0.00131995\n",
      " -0.00681459  0.01026187  0.00727064  0.00680899 -0.00685101  0.00308363\n",
      "  0.00295995  0.01494657 -0.00439323 -0.00472944  0.00297155  0.00873008\n",
      " -0.00147074 -0.00215419  0.01273373  0.00193305  0.00365264 -0.00512774\n",
      "  0.00309595  0.00632189  0.00143012 -0.00363988  0.0037374   0.00046146\n",
      "  0.00350994 -0.00586481  0.00725871 -0.00218684  0.00810762 -0.00698695\n",
      "  0.00873297  0.00712803  0.00647181  0.00663983  0.02015444 -0.00936105\n",
      " -0.00424042 -0.00747422  0.00326832  0.00968535]\n",
      "与知识产权最接近的6个词: [('我国', 0.9329206943511963), ('项目', 0.9322775602340698), ('战略', 0.9322413206100464), ('改革', 0.9321242570877075), ('收购', 0.9317615628242493), ('有限公司', 0.9316765666007996)]\n"
     ]
    }
   ],
   "source": [
    "print(f\"涨停的词向量: {model.wv['涨停']}\")\n",
    "print(f\"与知识产权最接近的6个词: {model.wv.most_similar('知识产权', topn=6)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.利用PCA方法对词嵌入向量降至二维并可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)  # PCA\n",
    "KM = KMeans(n_clusters=2)  # KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-140-39f05b01a506>:10: MatplotlibDeprecationWarning: The 's' parameter of annotate() has been renamed 'text' since Matplotlib 3.3; support for the old name will be dropped two minor releases later.\n",
      "  plt.annotate(s=corpus[i], xy=(vector_[:, 0][i], vector_[:, 1][i]),\n",
      "E:\\anacondains\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:238: RuntimeWarning: Glyph 8722 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "E:\\anacondains\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:201: RuntimeWarning: Glyph 8722 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus = ['大气污染','水污染','噪音','基金','安全','煤炭','启动','造成','污染物','禁止','调查','政府']  #选中的单词\n",
    "vector = model.wv[corpus]   #嵌入词向量\n",
    "vector_ = pca.fit_transform(vector)  #降维\n",
    "y_ = KM.fit_predict(vector_)\n",
    "\n",
    "colors=np.random.rand(len(corpus))\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.scatter(vector_[:,0],vector_[:,1],c=colors,alpha=0.3,s=600)   #绘制散点图\n",
    "for i in range(len(corpus)):    #给每个点进行中文标注\n",
    "    plt.annotate(s=corpus[i], xy=(vector_[:, 0][i], vector_[:, 1][i]),\n",
    "                 xytext=(vector_[:, 0][i]-0.015, vector_[:, 1][i]-0.001))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**可以看到“污染物”、“水污染”，“煤炭”之间的距离较近，“基金”与其他词距离较远，word2vec是可以有效提取到语义的**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
